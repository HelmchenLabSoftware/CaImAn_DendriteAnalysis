{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess and motion correct 3D movies\n",
    "Step 1 of the Caiman processing pipeline for multi-layer two-photon calcium imaging movies. Assume movie format acquired using the Scope setup (i.e. different layers arranged as mosaic on top of each other)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Setup\n",
    "The first cells import the various Python modules required by the notebook. In particular, a number of modules are imported from the Caiman package. In addition, we also setup the environment so that everything works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check for unnecessary imports\n",
    "# Generic imports\n",
    "# from __future__ import absolute_import, division, print_function\n",
    "# from builtins import *\n",
    "from __future__ import print_function\n",
    "\n",
    "import os, sys, glob, platform, shutil, re, math\n",
    "import json\n",
    "import time, datetime\n",
    "from functools import partial\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.transform\n",
    "from scipy import interpolate\n",
    "from tifffile import TiffFile, imread, imsave\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Import Bokeh library\n",
    "from bokeh.plotting import Figure, show\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.models import Range1d, CrosshairTool, HoverTool, Legend\n",
    "from bokeh.io import output_notebook, export_svgs\n",
    "from bokeh.models.sources import ColumnDataSource\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on Linux we have to add the caiman folder to Pythonpath\n",
    "if platform.system() == 'Linux':\n",
    "    sys.path.append(os.path.expanduser('~/caiman'))\n",
    "# environment variables for parallel processing\n",
    "os.environ['MKL_NUM_THREADS']='1'\n",
    "os.environ['OPENBLAS_NUM_THREADS']='1'\n",
    "os.environ['VECLIB_MAXIMUM_THREADS']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CaImAn and custom functions\n",
    "import caiman as cm\n",
    "from caiman.motion_correction import MotionCorrect\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This has to be in a separate cell, otherwise it wont work.\n",
    "from bokeh import resources\n",
    "output_notebook(resources=resources.INLINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify experimental parameters\n",
    "\n",
    "**Update!**\n",
    "\n",
    "Here we specify where the files are located, how the files are called, the frame rate of the acquisition, how to crop the movies and how many files to process.\n",
    "- animal_folder ... the folder where the data is located on the volume `Data`.\n",
    "- day_folder ...\n",
    "- area_folder ...\n",
    "- max_files ... how many files to process per session (0 for all)\n",
    "- n_planes ... number of planes recorded in the movies\n",
    "- x_crop ... fraction of x pixels to crop (i.e. 0.5 for half or >1 to specify the exact number of pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_folder = 'M3_October_2018'\n",
    "day_folder = 'M3_2018-10-02'\n",
    "area_folder = 'S1'\n",
    "max_sessions = 10 # how many sessions to process per area\n",
    "\n",
    "n_planes = 3 # number of planes in movie\n",
    "x_crop = 0.5 # fraction of x pixels to crop (i.e. 0.5 for half)\n",
    "\n",
    "max_group_size = 75 # if there are more files, they will be processed in groups\n",
    "\n",
    "n_processes = 4 # number of parallel processes (None to select automatically)\n",
    "\n",
    "# create the complete path to the data folder\n",
    "if platform.system() == 'Linux':\n",
    "    data_folder = '/home/ubuntu/Data'\n",
    "elif platform.system() == 'Darwin':\n",
    "    data_folder = '/Users/Henry/Data/temp/Dendrites_Gwen'\n",
    "data_folder = os.path.join(data_folder, animal_folder, day_folder, area_folder)\n",
    "data_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# select sessions for processing\n",
    "p = re.compile('\\d\\d-\\d\\d-\\d\\d_Live') # regular expression that should match the folder names (ie. 01-23-45_Live)\n",
    "sessions = [os.path.join(data_folder, x) for (i,x) in enumerate(sorted(os.listdir(data_folder))) if p.match(x) and i <= max_sessions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_files = []\n",
    "xml_files = []\n",
    "for i_session in sessions:\n",
    "    tiff_files.append([os.path.join(i_session, x) for x in os.listdir(i_session) if x.endswith('.tif') and not 'stacked' in x][0])\n",
    "    xml_files.append([os.path.join(i_session, x) for x in os.listdir(i_session) if 'parameters.xml' in x][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read frame rate from parameters.xml\n",
    "frame_rates = []\n",
    "for ix, i_session in enumerate(sessions):\n",
    "    tree = ET.parse(xml_files[ix])\n",
    "    root = tree.getroot()\n",
    "    for child in root:\n",
    "        if child.tag == 'area0': # Note: only area 0!\n",
    "            fr = child.find('Framerate_Hz')\n",
    "            frame_rate = float(fr.text)\n",
    "            frame_rates.append(frame_rate)\n",
    "    if ix <= 10:\n",
    "        print('Frame rate: %1.4f Hz (%s)' % (frame_rate, os.path.split(i_session)[1]))\n",
    "if ix > 10:\n",
    "    print('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup cluster\n",
    "The default backend mode for parallel processing is through the multiprocessing package. This will allow us to use all the cores in the VM. Note that the `cropTif` function has to be defined before starting the cluster, so that pool workers have the function available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the cluster (if a cluster already exists terminate it)\n",
    "if 'dview' in locals():\n",
    "    dview.terminate()\n",
    "c, dview, n_processes = cm.cluster.setup_cluster(\n",
    "    backend='local', n_processes=n_processes, single_thread=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert TIFF files to ImageJ hyperstack files\n",
    "This cell calls the `mosaic_to_stack` function in `utils` through the multiprocessing `starmap` method to make use of multiple cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert multiple planes in mosaic to 3D stack and save as IJ hyperstack\n",
    "stacked_files = dview.starmap(utils.mosaic_to_stack, [(x, n_planes, x_crop) for x in tiff_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Processing %1.0f files:' % (len(stacked_files)))\n",
    "print(*stacked_files[:10], sep='\\n')\n",
    "if len(stacked_files) > 10:\n",
    "    print('...')\n",
    "    print(*stacked_files[-5:], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join cropped TIF files\n",
    "Next, we create a large joined TIF file from individual cropped files. Further processing will be done on the joined file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = math.ceil(len(stacked_files) / float(max_group_size))\n",
    "files_per_group = math.ceil(len(stacked_files) / groups)\n",
    "stacked_files_by_group = []\n",
    "print('Processing files in %d groups' % (groups))\n",
    "for i_groups in range(int(groups)):\n",
    "    start_ix = int(i_groups * files_per_group)\n",
    "    stop_ix = int((i_groups+1) * files_per_group)\n",
    "    stacked_files_by_group.append(stacked_files[start_ix:stop_ix])\n",
    "    \n",
    "    print('Group %d (%d - %d): %d files' % (i_groups+1, start_ix, stop_ix, len(stacked_files[start_ix:stop_ix])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_files_by_group[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = cm.load_movie_chain(stacked_files_by_group[0], is3D=True, outtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies[:,1,:,:].play(backend='notebook', fr=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_tif_list = []\n",
    "json_fname_list = []\n",
    "total_frames_list = []\n",
    "trial_indices_list = []\n",
    "for ix, tiff_files_crop_group in enumerate(tiff_files_by_group):\n",
    "    # load movies\n",
    "    movies = cm.load(tiff_files_crop_group, outtype=np.int16)\n",
    "    \n",
    "    total_frames = movies.shape[0]\n",
    "    total_frames_list.append(total_frames)\n",
    "    dims = (movies.shape[1], movies.shape[2])\n",
    "    # derive joined file name and save\n",
    "    joined_tif = '%s_%s_Join_G%d_%d_crop.tif' % (date_folder, session_folder, ix, total_frames)\n",
    "    imsave(os.path.join(data_folder, joined_tif), movies)\n",
    "    frames_per_movie = dview.map(utils.getFramesTif, tiff_files_crop_group)\n",
    "    \n",
    "    movies = None # free the memory\n",
    "    \n",
    "    # trial index for each frame\n",
    "    trial_indices = []\n",
    "    for ix, frame_count in enumerate(frames_per_movie):\n",
    "        trial_indices = trial_indices + [ix]*frame_count\n",
    "    trial_indices_list.append(trial_indices)\n",
    "    \n",
    "    # create a Json file with information about source files\n",
    "    meta = {\"joined_file\": joined_tif, \n",
    "            \"source_frames\": frames_per_movie, \n",
    "            \"source_file\": [x.replace(data_folder + os.path.sep,'') for x in tiff_files_crop_group],\n",
    "            \"trial_index\": trial_indices,\n",
    "            \"frame_rate\": frame_rate\n",
    "           }\n",
    "    json_fname = joined_tif.replace('.tif','.json')\n",
    "    with open(os.path.join(data_folder, json_fname), 'w') as fid:\n",
    "        json.dump(meta, fid)\n",
    "    \n",
    "    # save output file names in list\n",
    "    joined_tif_list.append(joined_tif)\n",
    "    json_fname_list.append(json_fname)\n",
    "    \n",
    "    print('Saved joined TIF file %s' % (joined_tif))\n",
    "    print('Created JSON metadata file %s' % (json_fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display average signal intensity\n",
    "This step is optional and useful as sanity check how the imported data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select group (0, 1, ...)\n",
    "group_ix = 0\n",
    "\n",
    "mov = cm.load(os.path.join(data_folder, joined_tif_list[group_ix]), outtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot average signal intensity per frame\n",
    "frame_avg = np.mean(np.mean(mov, axis=1), axis=1)\n",
    "frames = np.array(range(len(frame_avg)))\n",
    "\n",
    "trial_names = [x.replace(data_folder + os.path.sep,'').replace('.tif','').replace('_crop','') \n",
    "               for x in tiff_files_by_group[group_ix]]\n",
    "trial_names_frames = [trial_names[x] for x in trial_indices_list[group_ix]]\n",
    "\n",
    "data = {'x': frames, \n",
    "        'y': frame_avg,\n",
    "        'trial_idx': trial_indices_list[group_ix],\n",
    "        'trial_name': trial_names_frames\n",
    "       }\n",
    "data_source = ColumnDataSource(data)\n",
    "\n",
    "p = Figure(plot_width=900, plot_height=300, title=('Frame average - Group %d' % (group_ix))) \n",
    "p.add_tools(CrosshairTool(), utils.getHover())\n",
    "p.line('x', 'y', source=data_source, line_width=2, legend='Original', color='blue')\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust image intensity\n",
    "Optional step to adjust image intensity for each trial. This is useful when the average intensity fluctuates a lot from trial to trial, as shown by the previous plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix, joined_tif in enumerate(joined_tif_list): \n",
    "    mov = cm.load(os.path.join(data_folder, joined_tif), outtype=np.int16)\n",
    "    n_trials = max(trial_indices_list[ix])\n",
    "\n",
    "    for i_trial in range(n_trials+1):\n",
    "        trial_idx = np.where(np.array(trial_indices_list[ix]) == i_trial)[0]\n",
    "        # subtract average intensity\n",
    "        mov[trial_idx,:,:] = mov[trial_idx,:,:] - np.mean(mov[trial_idx,:,:])\n",
    "    imsave(os.path.join(data_folder, joined_tif), mov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motion correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, setup the parameters for motion correction. The following parameters influence the **quality** of the motion correction:\n",
    "- niter_rig ... number of iterations for rigid registration (larger = better). Little improvement likely above 5-10.\n",
    "- strides ... intervals at which patches are laid out for motion correction (smaller = better)\n",
    "- overlaps ... overlap between patches\n",
    "\n",
    "Note that smaller values for strides / overlap will improve registration but also lead to NaNs in the output image. In general, there is a trade-off between the quality of registration and the presence / number of NaNs in the output (at least if there is significant motion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for motion correction\n",
    "params = {'niter_rig': 5,\n",
    "          'max_shifts': (int(np.round(dims[0]/10)), int(np.round(dims[1]/10))),  # maximum allow rigid shift\n",
    "          # for parallelization split the movies in  num_splits chuncks across time\n",
    "          'splits_rig': 50,\n",
    "          # if none all the splits are processed and the movie is saved\n",
    "          'num_splits_to_process_rig': None,\n",
    "          # intervals at which patches are laid out for motion correction\n",
    "          'strides': (24, 24),\n",
    "          # overlap between pathes (size of patch strides+overlaps)\n",
    "          'overlaps': (24, 24),\n",
    "          # for parallelization split the movies in  num_splits chuncks across time\n",
    "          'splits_els': 50,\n",
    "          # if none all the splits are processed and the movie is saved\n",
    "          'num_splits_to_process_els': [28, None],\n",
    "          'upsample_factor_grid': 4,  # upsample factor to avoid smearing when merging patches\n",
    "          # maximum deviation allowed for patch with respect to rigid shift\n",
    "          'max_deviation_rigid': 3,\n",
    "          # Specifies how to deal with borders. (True, False, 'copy', 'min')\n",
    "          'border_nan': False,\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also some parameters for computing the quality metrics. These probably don't have to be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for computing metrics\n",
    "winsize = 100\n",
    "swap_dim = False\n",
    "resize_fact_flow = 0.2    # downsample for computing ROF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define some functions. See the function doc strings for further information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupMC(fname, params):\n",
    "    \"\"\"\n",
    "    Configure motion correction oject mc with input filename and parameters.\n",
    "    \n",
    "    Return mc\n",
    "    \"\"\"\n",
    "    mov = cm.load(fname)\n",
    "    mc = MotionCorrect(fname, mov.min(), dview=dview, \n",
    "                       max_shifts=params['max_shifts'], \n",
    "                       niter_rig=params['niter_rig'], \n",
    "                       splits_rig=params['splits_rig'],\n",
    "                       num_splits_to_process_rig=params['num_splits_to_process_rig'], \n",
    "                       strides= params['strides'], \n",
    "                       overlaps= params['overlaps'], \n",
    "                       splits_els=params['splits_els'],\n",
    "                       num_splits_to_process_els=params['num_splits_to_process_els'], \n",
    "                       upsample_factor_grid=params['upsample_factor_grid'], \n",
    "                       max_deviation_rigid=params['max_deviation_rigid'],\n",
    "                       border_nan=params['border_nan'],\n",
    "                       shifts_opencv = True, \n",
    "                       nonneg_movie = False)\n",
    "    return mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolateNans(frame, n=10):\n",
    "    \"\"\"\n",
    "    Interpolate NaN values in frame with average of n closest non-nan pixels\n",
    "    \n",
    "    Return interpolated frame\n",
    "    \"\"\"\n",
    "    frame_interp = frame.copy()\n",
    "    # indices for all NaN pixels\n",
    "    nan_pixel = np.array(np.where(np.isnan(frame))).T\n",
    "    if not len(nan_pixel):\n",
    "        return frame\n",
    "    # indices for all non-NaN pixels\n",
    "    valid_pixel = np.array(np.where(~np.isnan(frame))).T\n",
    "    for pix in nan_pixel:\n",
    "        # distance between NaN pixel and all valid pixels\n",
    "        dist = np.linalg.norm(valid_pixel - pix, axis=1)\n",
    "        # find the closest pixels and get their values in frame\n",
    "        closest_pixel = valid_pixel[np.argsort(dist)[:n],:]\n",
    "        closest_pixel_vals = frame[closest_pixel[:,0],closest_pixel[:,1]]\n",
    "        # replace NaN with average\n",
    "        frame_interp[pix[0],pix[1]] = np.mean(closest_pixel_vals)\n",
    "\n",
    "    return frame_interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeMetrics(mc, bord_px_els, swap_dim, winsize, resize_fact_flow):\n",
    "    \"\"\"\n",
    "    Compute the quality metrics for the registration.\n",
    "    \"\"\"\n",
    "    \n",
    "    final_size = np.subtract(mc.total_template_els.shape, bord_px_els) # remove pixels in the boundaries\n",
    "    \n",
    "    tmpl_rig, corr_orig, flows_orig, norms_orig, crispness_orig = \\\n",
    "    cm.motion_correction.compute_metrics_motion_correction(mc.fname[0], final_size[0], final_size[1],\n",
    "                                                           swap_dim, winsize=winsize, play_flow=False, \n",
    "                                                           resize_fact_flow=resize_fact_flow)\n",
    "\n",
    "    tmpl_rig, corr_rig, flows_rig, norms_rig, crispness_rig = \\\n",
    "    cm.motion_correction.compute_metrics_motion_correction(mc.fname_tot_rig[0], final_size[0], final_size[1],\n",
    "                                                           swap_dim, winsize=winsize, play_flow=False, \n",
    "                                                           resize_fact_flow=resize_fact_flow)\n",
    "\n",
    "    tmpl_els, corr_els, flows_els, norms_els, crispness_els = \\\n",
    "    cm.motion_correction.compute_metrics_motion_correction(mc.fname_tot_els[0], final_size[0], final_size[1],\n",
    "                                                           swap_dim, winsize=winsize, play_flow=False, \n",
    "                                                           resize_fact_flow=resize_fact_flow)\n",
    "    \n",
    "    metrics = {\n",
    "        'tmpl_rig': tmpl_rig,\n",
    "        'corr_orig': corr_orig,\n",
    "        'flows_orig': flows_orig,\n",
    "        'crispness_orig': crispness_orig,\n",
    "        'norms_orig': norms_orig,\n",
    "        \n",
    "        'corr_rig': corr_rig,\n",
    "        'flows_rig': flows_rig,\n",
    "        'crispness_rig': crispness_rig,\n",
    "        'norms_rig': norms_rig,\n",
    "        \n",
    "        'tmpl_els': tmpl_els,\n",
    "        'corr_els': corr_els,\n",
    "        'flows_els': flows_els,\n",
    "        'crispness_els': crispness_els,\n",
    "        'norms_els': norms_els,\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeBoundaryPixels(movie, mc, mc_type):\n",
    "    \"\"\"\n",
    "    Remove the boundary pixels corresponding to the max. shift of the registration.\n",
    "    \"\"\"\n",
    "    # compute borders to exclude\n",
    "    if mc_type == 'els':\n",
    "        bord_px = np.ceil(np.maximum(np.max(np.abs(mc.x_shifts_els)), np.max(np.abs(mc.y_shifts_els)))).astype(np.int)\n",
    "        final_size = np.subtract(mc.total_template_els.shape, bord_px)\n",
    "    elif mc_type == 'rig':\n",
    "        bord_px = np.ceil(np.max(mc.shifts_rig)).astype(np.int)\n",
    "        final_size = np.subtract(mc.total_template_rig.shape, bord_px)\n",
    "    \n",
    "    # remove pixels in the boundaries\n",
    "    final_size_x = final_size[0]\n",
    "    final_size_y = final_size[1]\n",
    "    max_shft_x = np.int(np.ceil((np.shape(movie)[1] - final_size_x) / 2))\n",
    "    max_shft_y = np.int(np.ceil((np.shape(movie)[2] - final_size_y) / 2))\n",
    "    max_shft_x_1 = - ((np.shape(movie)[1] - max_shft_x) - (final_size_x))\n",
    "    max_shft_y_1 = - ((np.shape(movie)[2] - max_shft_y) - (final_size_y))\n",
    "    if max_shft_x_1 == 0:\n",
    "        max_shft_x_1 = None\n",
    "\n",
    "    if max_shft_y_1 == 0:\n",
    "        max_shft_y_1 = None\n",
    "    \n",
    "    movie = movie[:, max_shft_x:max_shft_x_1, max_shft_y:max_shft_y_1]\n",
    "    \n",
    "    if mc_type == 'els':\n",
    "        mc.total_template_els = mc.total_template_els[max_shft_x:max_shft_x_1, max_shft_y:max_shft_y_1]\n",
    "    elif mc_type == 'rig':\n",
    "        mc.total_template_rig = mc.total_template_els[max_shft_x:max_shft_x_1, max_shft_y:max_shft_y_1]\n",
    "    \n",
    "    return movie, mc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to run motion correction for the joined TIF file. If there are a lot of concatenated trials, this might take a while to complete.\n",
    "\n",
    "The following outputs will be saved:\n",
    "- result of rigid motion correction in Python mmap format and as TIF file\n",
    "- result of pw-rigid motion correction in Python mmap format and as TIF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runMotionCorrection(fname, params):\n",
    "    \"\"\"\n",
    "    Run motion correction for single input file fname using parameters in params. \n",
    "    \n",
    "    Return motion correction object mc\n",
    "    \"\"\"\n",
    "    \n",
    "    interp_nans = False\n",
    "    \n",
    "    # create mc object\n",
    "    mc = setupMC(fname, params)\n",
    "    \n",
    "    # apply rigid correction\n",
    "    mc.motion_correct_rigid(save_movie=True)\n",
    "    \n",
    "    # apply pw-rigid correction\n",
    "    mc.motion_correct_pwrigid(save_movie=True, template=mc.total_template_rig, show_template = False)\n",
    "    \n",
    "    # load corrected movie - els\n",
    "    mov_els = cm.load(mc.fname_tot_els[0])\n",
    "    # remove boundary pixels\n",
    "    mov_els, mc = removeBoundaryPixels(mov_els, mc, 'els')\n",
    "    \n",
    "    # create copy to interpolate NaNs\n",
    "    mov_els_copy = mov_els.copy()\n",
    "    if interp_nans:\n",
    "        for ix in range(mov_els.shape[0]):\n",
    "            mov_els_copy[ix,:,:] = interpolateNans(mov_els[ix,:,:])\n",
    "    \n",
    "    # save pw-rigid corrected movies as TIF\n",
    "    dummy_fname = 'dummy_%s.tif' % (datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "    dummy_fname = os.path.join(data_folder, dummy_fname)\n",
    "    imsave(dummy_fname, mov_els_copy)\n",
    "    \n",
    "    # save pw-rigid corrected and interpolated movie as mmap\n",
    "    base_name = mc.fname_tot_rig[0][:mc.fname_tot_rig[0].find('_rig_')] + '_els_'\n",
    "    fname_new = cm.save_memmap([dummy_fname], base_name=base_name, order='F')\n",
    "    \n",
    "    # remove previous mmap file and rename TIF file\n",
    "    os.remove(mc.fname_tot_els[0])\n",
    "    mc.fname_tot_els = [fname_new]\n",
    "    os.rename(dummy_fname, fname_new.replace('.mmap', '.tif'))\n",
    "    \n",
    "    # save rigid corrected movies as TIF\n",
    "    imsave(mc.fname_tot_rig[0].replace('.mmap','.tif'), cm.load(mc.fname_tot_rig[0]))\n",
    "    \n",
    "    return mc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation with template - uncorrected data\n",
    "First, we just get the correlation with the template for the uncorrected data. This can be used to re-assign groups, for example. This part can also be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mc_init_list = []\n",
    "corr_orig_list = []\n",
    "\n",
    "t_start = time.time()\n",
    "for i_file in joined_tif_list:\n",
    "    fname = os.path.join(data_folder, i_file)\n",
    "    mc_init = setupMC(fname, params)\n",
    "    \n",
    "    # compute initial template by binned median filtering\n",
    "    template = cm.load(fname).bin_median(window=10)\n",
    "    \n",
    "    # compute metric for orig\n",
    "    tmpl, corr_orig, flows_orig, norms_orig, crispness_orig = \\\n",
    "    cm.motion_correction.compute_metrics_motion_correction(mc_init.fname[0], template.shape[0], template.shape[1], \n",
    "                                                       swap_dim, winsize=winsize, play_flow=False, \n",
    "                                                       resize_fact_flow=0.01)\n",
    "    \n",
    "    \n",
    "    \n",
    "    mc_init_list.append(mc_init)\n",
    "    corr_orig_list.append(corr_orig)\n",
    "    \n",
    "    clear_output()\n",
    "    \n",
    "# print elapsed time\n",
    "t_elapsed = time.time() - t_start\n",
    "print('\\nFinished pre-MC in %1.2f s (%1.2e s per frame)' % (t_elapsed, t_elapsed/sum(total_frames_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# select group (0, 1, ...)\n",
    "group_ix = 0\n",
    "\n",
    "trial_names = [x.replace(data_folder + os.path.sep,'').replace('_crop.tif','') for x in tiff_files_by_group[group_ix]]\n",
    "trial_names_frames = [trial_names[x] for x in trial_indices_list[group_ix]]\n",
    "\n",
    "frames = np.array(range(len(corr_orig_list[group_ix])))\n",
    "data = {'x': list(range(len(corr_orig_list[group_ix]))), \n",
    "        'y': corr_orig_list[group_ix],\n",
    "        'trial_idx': trial_indices_list[group_ix],\n",
    "        'trial_name': trial_names_frames\n",
    "       }\n",
    "data_source = ColumnDataSource(data)\n",
    "\n",
    "p = Figure(plot_width=900, plot_height=300, title=('Correlation with template - Group %d' % (group_ix))) \n",
    "p.add_tools(CrosshairTool(), utils.getHover())\n",
    "p.line('x', 'y', source=data_source, line_width=2, legend='Original', color='blue')\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the motion correction\n",
    "Next, we run the MC itself. This can be time consuming for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t_start = time.time()\n",
    "mc_list = []\n",
    "for i_file in joined_tif_list:\n",
    "    \n",
    "    fname = os.path.join(data_folder, i_file)\n",
    "    \n",
    "    mc = runMotionCorrection(fname, params)\n",
    "    \n",
    "    mc_list.append(mc)\n",
    "    \n",
    "clear_output()\n",
    "\n",
    "# print elapsed time\n",
    "t_elapsed = time.time() - t_start\n",
    "print('\\nFinished MC in %1.2f s (%1.2f s per frame)' % (t_elapsed, t_elapsed/sum(total_frames_list)))\n",
    "\n",
    "if platform.system() == 'Darwin':\n",
    "    os.system('say \"your program has finished\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess quality of motion correction\n",
    "A number of key metrics can be calculated to assess how much motion correction improved overall motion. \n",
    "1. Correlation\n",
    "Correlations of each frame with the template image (binned median) for original, rigid correction and pw-rigid correction. The mean correlation gives an overall impression of motion. The minimum correlation indicates the parts of the movie that are worst affected by motion. Larger correlations indicate less motion.\n",
    "2. Crispness\n",
    "Crispness provides a measure of the smoothness of the corrected average image. Intuitively, a dataset with nonregistered motion will have a blurred mean image, resulting in a lower value for the total gradient field norm. Thus, larger values indicate a crisper average image and less residual motion. Crispness is calculated from the gradient field of the mean image (`np.gradient`).\n",
    "3. Residual optical flow\n",
    "Optic flow algorithms attempt to match each frame to the template by estimating locally smooth displacement fields. The output is an image where each pixel described the local displacement between template and frame at this point. The smaller the local displacement, the better the registration. Here we compute the matrix norm of the optic flow matrix as summary statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# compute quality assessment metrics\n",
    "crispness = []\n",
    "norms = []\n",
    "corr_mean = []\n",
    "corr_min = []\n",
    "metrics = []\n",
    "for mc in mc_list:\n",
    "    bord_px = np.ceil(np.maximum(np.max(np.abs(mc.x_shifts_els)), np.max(np.abs(mc.y_shifts_els)))).astype(np.int)\n",
    "    mtrs = computeMetrics(mc, bord_px, swap_dim, winsize, resize_fact_flow)\n",
    "    metrics.append(mtrs)\n",
    "    # correlations, crispness and norms of residual optic flow as indicators of registration quality\n",
    "    crispness.append(np.array([mtrs['crispness_orig'], mtrs['crispness_rig'], mtrs['crispness_els']]))\n",
    "    norms.append(np.array([np.mean(mtrs['norms_orig']), np.mean(mtrs['norms_rig']), np.mean(mtrs['norms_els'])]))\n",
    "    corr_mean.append(np.array([np.mean(mtrs['corr_orig']), np.mean(mtrs['corr_rig']), np.mean(mtrs['corr_els'])]))\n",
    "    corr_min.append(np.array([np.min(mtrs['corr_orig']), np.min(mtrs['corr_rig']), np.min(mtrs['corr_els'])]))\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print different metrics for raw movie and rigid / pw-rigid corrected movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix in range(len(mc_list)):\n",
    "    print('MC evaluation - Group %d:' % (ix))\n",
    "    if corr_mean[ix][0] > corr_mean[ix][1] or corr_mean[ix][0] > corr_mean[ix][2]:\n",
    "        print('\\x1b[1;03;31m'+'Mean corr - raw / rigid / pw_rigid: ' \n",
    "              + str(['{:.2f}'.format(i) for i in corr_mean[ix]]) + '\\x1b[0m')\n",
    "    else:\n",
    "        print('Mean corr - raw / rigid / pw_rigid: ' + str(['{:.2f}'.format(i) for i in corr_mean[ix]]))\n",
    "\n",
    "    if corr_min[ix][0] > corr_min[ix][1] or corr_min[ix][0] > corr_min[ix][2]:\n",
    "        print('\\x1b[1;03;31m'+'Min corr - raw / rigid / pw_rigid: ' \n",
    "              + str(['{:.2f}'.format(i) for i in corr_min[ix]])+ '\\x1b[0m')\n",
    "    else:\n",
    "        print('Min corr - raw / rigid / pw_rigid: ' + str(['{:.2f}'.format(i) for i in corr_min[ix]]))\n",
    "    if crispness[ix][0] > crispness[ix][1] or crispness[ix][0] > crispness[ix][2]:\n",
    "        print('\\x1b[1;03;31m'+'Crispness - raw / rigid / pw_rigid: ' \n",
    "              + str(['{:.0f}'.format(i) for i in crispness[ix]]) + '\\x1b[0m')\n",
    "    else:\n",
    "        print('Crispness - raw / rigid / pw_rigid: ' + str(['{:.0f}'.format(i) for i in crispness[ix]]))\n",
    "    if norms[ix][0] < norms[ix][1] or norms[ix][0] < norms[ix][2]:\n",
    "        print('\\x1b[1;03;31m'+'Norms - raw / rigid / pw_rigid: ' \n",
    "              + str(['{:.0f}'.format(i) for i in norms[ix]]) + '\\x1b[0m')\n",
    "    else:\n",
    "        print('Norms - raw / rigid / pw_rigid: ' + str(['{:.2f}'.format(i) for i in norms[ix]]))\n",
    "        \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation with template image\n",
    "Plot correlations of each frame with the template image (binned median) for original, rigid correction and pw-rigid correction. The bokeh plotting library provides a toolbar for interaction with the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select group (0, 1, ...)\n",
    "group_ix = 0\n",
    "\n",
    "p1 = Figure(plot_width=900, plot_height=300, title=('Correlation with template - Group %d' % (group_ix))) \n",
    "frames = np.array(range(len(metrics[group_ix]['corr_orig'])))\n",
    "p1.line(frames,np.array(metrics[group_ix]['corr_orig']), line_width=2, legend='Original', color='blue')\n",
    "p1.line(frames,np.array(metrics[group_ix]['corr_rig']), line_width=2, legend='Rigid', color='orange')\n",
    "p1.line(frames,np.array(metrics[group_ix]['corr_els']), line_width=2, legend='PW-Rigid', color='green')\n",
    "\n",
    "p2 = Figure(plot_width=250, plot_height=250)\n",
    "p2.circle(np.array(metrics[group_ix]['corr_orig']), np.array(metrics[group_ix]['corr_rig']), size=5)\n",
    "p2.line([0,1],[0,1], line_width=1, color='black', line_dash='dashed')\n",
    "p2.xaxis.axis_label = 'Original'\n",
    "p2.yaxis.axis_label = 'Rigid'\n",
    "\n",
    "p3 = Figure(plot_width=250, plot_height=250)\n",
    "p3.circle(np.array(metrics[group_ix]['corr_rig']), np.array(metrics[group_ix]['corr_els']), size=5)\n",
    "p3.line([0,1],[0,1], line_width=1, color='black', line_dash='dashed')\n",
    "p3.xaxis.axis_label = 'Rigid'\n",
    "p3.yaxis.axis_label = 'PW-Rigid'\n",
    "\n",
    "# make a grid\n",
    "grid = gridplot([[p1, None], [p2, p3]], sizing_mode='fixed', toolbar_location='left')\n",
    "\n",
    "show(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residual optic flow\n",
    "Optic flow algorithms attempt to match each frame to the template by estimating locally smooth displacement fields. The output is an image where each pixel described the local displacement between template and frame at this point. The smaller the local displacement, the better the registration. Norms are the matrix norms of the optic flow matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select group (0, 1, ...)\n",
    "group_ix = 0\n",
    "\n",
    "# plot the results of Residual Optical Flow\n",
    "metrics_files = [mc_list[group_ix].fname_tot_els[0][:-4] + '_metrics.npz', \n",
    "                 mc_list[group_ix].fname_tot_rig[0][:-4] + '_metrics.npz', \n",
    "                 mc_list[group_ix].fname[0][:-4] + '_metrics.npz']\n",
    "\n",
    "plt.figure(figsize = (20,10))\n",
    "for cnt, fl, metr in zip(range(len(metrics_files)),metrics_files,['pw_rigid','rigid','raw']):\n",
    "    with np.load(fl) as ld:\n",
    "        print('Correction: %s' % (metr))\n",
    "        print('Norms: %1.2f +- %1.2f' % (np.mean(ld['norms']), np.std(ld['norms'])))\n",
    "        \n",
    "        plt.subplot(len(metrics_files), 3, 1 + 3 * cnt)\n",
    "        plt.ylabel(metr)\n",
    "                   \n",
    "        if metr == 'raw':\n",
    "            mean_img = np.mean(cm.load(mc.fname[0]), axis=0)\n",
    "        elif metr == 'rigid':\n",
    "            mean_img = np.mean(cm.load(mc.fname_tot_rig[0]), axis=0)\n",
    "        elif metr == 'pw_rigid':\n",
    "            mean_img = np.mean(cm.load(mc.fname_tot_els[0]), axis=0)\n",
    "        \n",
    "        lq, hq = np.nanpercentile(mean_img, [.5, 99.5])\n",
    "        plt.imshow(mean_img, vmin=lq, vmax=hq, cmap='gray')\n",
    "        if not cnt:\n",
    "            plt.title('Mean')\n",
    "        plt.subplot(len(metrics_files), 3, 3 * cnt + 2)\n",
    "        plt.imshow(ld['img_corr'], vmin=0, vmax=.5, cmap='gray')\n",
    "        if not cnt:\n",
    "            plt.title('Correlation image')\n",
    "        plt.subplot(len(metrics_files), 3, 3 * cnt + 3)\n",
    "        flows = ld['flows']\n",
    "        plt.imshow(np.mean(np.sqrt(flows[:, :, :, 0]**2 + flows[:, :, :, 1]**2), 0), vmin=0, vmax=0.5, cmap='gray')\n",
    "        plt.colorbar()\n",
    "        if not cnt:\n",
    "            plt.title('Mean optical flow')\n",
    "plt.suptitle('Residual Optic Flow - Group %d' % (group_ix), fontsize=22);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect frames with bad motion\n",
    "Identify frames with significant residual motion (low correlation with template). Write a JSON file with criterion and indices of frames matching the criterion. This file can be used in further analysis to exclude the frames corrupted by motion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeJsonBadFrames(criterion, thresh, frame_ix, mc, mc_type, data_folder):\n",
    "    exclude_info = {\"criterion\": criterion, \n",
    "        \"threshold:\": thresh, \n",
    "        \"frames\": frame_ix}\n",
    "    if mc_type == 'els':\n",
    "        json_fname = mc.fname_tot_els[0].replace('.mmap','') + 'badFrames' + '.json'\n",
    "    elif mc_type == 'rig':\n",
    "        json_fname = mc.fname_tot_rig[0].replace('.mmap','') + 'badFrames' + '.json'\n",
    "    with open(os.path.join(data_folder, json_fname), 'w') as fid:\n",
    "        json.dump(exclude_info, fid)\n",
    "    print('Created JSON metadata file %s' % (json_fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = [0.4] # find frames where value is less than criterion (one value per group)\n",
    "\n",
    "for i_thr in range(len(thresh)):\n",
    "    print('Group %d' % (i_thr))\n",
    "    # pw-rigid registration\n",
    "    criterion = 'corr_els'\n",
    "    bad_frames = [ix for ix, i in enumerate(metrics[i_thr][criterion]) if i < thresh[i_thr]]\n",
    "    print('%1.0f frames matching criterion after pw-rigid registration.' % (len(bad_frames)))\n",
    "    writeJsonBadFrames(criterion, thresh[i_thr], bad_frames, mc_list[i_thr], 'els', data_folder)\n",
    "    # rigid registration\n",
    "    criterion = 'corr_rig'\n",
    "    bad_frames = [ix for ix, i in enumerate(metrics[i_thr][criterion]) if i < thresh[i_thr]]\n",
    "    print('\\n%1.0f frames matching criterion after rigid registration.' % (len(bad_frames)))\n",
    "    writeJsonBadFrames(criterion, thresh[i_thr], bad_frames, mc_list[i_thr], 'rig', data_folder)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caiman",
   "language": "python",
   "name": "caiman"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
