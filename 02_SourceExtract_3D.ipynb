{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CNMF source extraction on movies\n",
    "Step 2 of the Caiman processing pipeline for dendritic two-photon calcium imaging movies. This part uses mmap files as input. These are created during motion correction with the Caiman toolbox (see `01_Preprocess_MC_3D.ipynb`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Setup\n",
    "The first cells import the various Python modules required by the notebook. In particular, a number of modules are imported from the Caiman package. In addition, we also setup the environment so that everything works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic imports\n",
    "# from __future__ import absolute_import, division, print_function\n",
    "# from builtins import *\n",
    "\n",
    "import os, platform, glob, sys, re, copy\n",
    "import fnmatch\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import savemat\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Import Bokeh library\n",
    "import bokeh.plotting as plotting\n",
    "from bokeh.plotting import Figure, show\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.models import Range1d, CrosshairTool, HoverTool, Legend\n",
    "from bokeh.io import output_notebook, export_svgs\n",
    "from bokeh.models.sources import ColumnDataSource\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This has to be in a separate cell, otherwise it wont work.\n",
    "from bokeh import resources\n",
    "output_notebook(resources=resources.INLINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on Linux we have to add the caiman folder to Pythonpath\n",
    "if platform.system() == 'Linux':\n",
    "    sys.path.append(os.path.expanduser('~/caiman'))\n",
    "# environment variables for parallel processing\n",
    "os.environ['MKL_NUM_THREADS']='1'\n",
    "os.environ['OPENBLAS_NUM_THREADS']='1'\n",
    "os.environ['VECLIB_MAXIMUM_THREADS']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CaImAn imports\n",
    "import caiman as cm\n",
    "from caiman.source_extraction.cnmf import cnmf as cnmf\n",
    "from caiman.source_extraction.cnmf import params as params\n",
    "from caiman.components_evaluation import estimate_components_quality as estimate_q\n",
    "from caiman.components_evaluation import estimate_components_quality_auto\n",
    "from caiman.utils.visualization import plot_contours, nb_view_patches, nb_plot_contour\n",
    "from caiman.source_extraction.cnmf import utilities as cnmf_utils\n",
    "import caiman_utils as cm_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(cm_utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select files\n",
    "The following need to be specified:\n",
    "- data_folder ... location of the data (relative to ~/Data)\n",
    "- mc_output ... select if output of rigid ('rig') or piece-wise rigid ('els') motion correction should be used (currently only 'rig' is tested and works)\n",
    "- max_files ... maximum number of files to process, e.g. for testing (if 0, all files will be processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_folder = 'M3_October_2018'\n",
    "date_folder = 'M3_2018-10-02'\n",
    "session_folder = 'S1'\n",
    "group_id = 'G0'\n",
    "\n",
    "mc_output = 'rig'\n",
    "remove_bad_frames = True # remove bad frames specified in Json file\n",
    "\n",
    "# create the complete path to the data folder\n",
    "if platform.system() == 'Linux':\n",
    "    data_folder = '/home/ubuntu/Data'\n",
    "elif platform.system() == 'Darwin':\n",
    "    data_folder = '/Users/Henry/Data/temp/Dendrites_Gwen'\n",
    "data_folder = os.path.join(data_folder, animal_folder, date_folder, session_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the mmap file created during motion correction\n",
    "all_files = os.listdir(data_folder)\n",
    "mmap_files = sorted([x for x in all_files if x.startswith('%s_%s' % (date_folder, session_folder)) \n",
    "           and x.endswith('.mmap') and mc_output in x and group_id in x and not 'remFrames' in x])\n",
    "n_planes = len(mmap_files)\n",
    "\n",
    "print('Found %d mmap files. Check allocation to planes!' % (n_planes))\n",
    "for i_plane in range(n_planes):\n",
    "    print('Plane %d: %s' % (i_plane, mmap_files[i_plane]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmap_files = [os.path.join(data_folder, x) for x in mmap_files]\n",
    "# get metadata\n",
    "for file in os.listdir(data_folder):\n",
    "    if fnmatch.fnmatch(file, '%s_%s_Join_%s_*[!badFrames].json' % (date_folder, session_folder, group_id)):\n",
    "        meta = json.load(open(os.path.join(data_folder,file)))\n",
    "        break\n",
    "trial_index = np.array(meta['trial_index'])\n",
    "frame_rate = meta['frame_rate'] / n_planes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and remove bad frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_frames = np.array([], dtype='int64')\n",
    "for fname in mmap_files:\n",
    "    bad_frames = np.concatenate((bad_frames, cm_utils.getBadFrames(fname)))\n",
    "bad_frames = np.unique(bad_frames)\n",
    "bad_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_frames = np.array([], dtype='int64')\n",
    "fname_list = []\n",
    "images_list = []\n",
    "\n",
    "# first, create list of bad frame indices (for all planes combined)\n",
    "for fname in mmap_files:\n",
    "    bad_frames = np.concatenate((bad_frames, cm_utils.getBadFrames(fname)))\n",
    "bad_frames = np.unique(bad_frames)\n",
    "\n",
    "# remove the bad frames from all files\n",
    "for fname in mmap_files:\n",
    "    Yr, dims = cm_utils.loadData(fname)\n",
    "    images, Y, fname_rem = cm_utils.removeBadFrames(fname, trial_index, Yr, dims, bad_frames, data_folder)\n",
    "    print(Y.shape)\n",
    "    fname_list.append(fname_rem)\n",
    "    images_list.append(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display frame average for each plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,60))\n",
    "for ix_plane in range(n_planes):\n",
    "    avg_img = np.mean(images_list[ix_plane],axis=0)\n",
    "    plt.subplot(1,n_planes,ix_plane+1)\n",
    "    plt.imshow(avg_img, cmap='gray'), plt.title('Frame average - Plane %d' % (ix_plane));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify if plane contains dendritic signals\n",
    "CaImAn uses different initialization methods depending on whether the signals are dendritic or somatic. Therefore, we need to specify the types of signal expected in each plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_dendritic = [True, False, True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup cluster\n",
    "The default backend mode for parallel processing is through the multiprocessing package. This will allow us to use all the cores in the VM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the cluster (if a cluster already exists terminate it)\n",
    "n_processes = 4 # number of compute processes (None to select automatically)\n",
    "if 'dview' in locals() and dview is not None:\n",
    "    dview.terminate()\n",
    "c, dview, n_processes = cm.cluster.setup_cluster(\n",
    "    backend='local', n_processes=n_processes, single_thread=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for source extraction\n",
    "Next, we define the important parameters for calcium source extraction. These parameters will have to be iteratively refined for the respective datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset dependent parameters\n",
    "decay_time = 0.4                            # length of a typical transient in seconds\n",
    "\n",
    "# parameters for source extraction and deconvolution\n",
    "p = 1                       # order of the autoregressive system\n",
    "gnb = 2                     # number of global background components\n",
    "merge_thresh = 0.8          # merging threshold, max correlation allowed\n",
    "rf = [25, 50]                     # half-size of the patches in pixels. e.g., if rf=25, patches are 50x50\n",
    "stride_cnmf = 5             # amount of overlap between the patches in pixels\n",
    "K = 20                       # number of components per patch\n",
    "gSig = [4, 4]               # expected half size of neurons in pixels\n",
    "\n",
    "method_init = 'sparse_nmf'  # initialization method (if analyzing dendritic data use 'sparse_nmf', else 'greedy_roi')\n",
    "#alpha_snmf = 10e2           # sparsity penalty for dendritic data analysis through sparse NMF\n",
    "alpha_snmf = 100\n",
    "normalize_init = True      # default is True\n",
    "\n",
    "ssub = 1                    # spatial subsampling during initialization\n",
    "tsub = 1                    # temporal subsampling during intialization\n",
    "\n",
    "# parameters for component evaluation\n",
    "min_SNR = 2.0               # signal to noise ratio for accepting a component\n",
    "rval_thr = 0.85              # space correlation threshold for accepting a component\n",
    "cnn_thr = 0.99              # threshold for CNN based classifier\n",
    "cnn_lowest = 0.1 # neurons with cnn probability lower than this value are rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Parameters object\n",
    "# unspecified parameters get default values\n",
    "opts_dict = {'fnames': fname_rem,\n",
    "            'fr': frame_rate,\n",
    "            'decay_time': decay_time,\n",
    "            'p': 1,\n",
    "            'nb': gnb,\n",
    "            'rf': rf,\n",
    "            'K': K, \n",
    "            'stride': stride_cnmf,\n",
    "            'method_init': method_init,\n",
    "            'alpha_snmf': alpha_snmf,\n",
    "             'normalize_init': normalize_init,\n",
    "            'rolling_sum': True,\n",
    "            'only_init': True,\n",
    "            'ssub': ssub,\n",
    "            'tsub': tsub,\n",
    "            'min_SNR': min_SNR,\n",
    "            'rval_thr': rval_thr,\n",
    "            'use_cnn': True,\n",
    "            'min_cnn_thr': cnn_thr,\n",
    "            'cnn_lowest': cnn_lowest}\n",
    "\n",
    "opts = params.CNMFParams(params_dict=opts_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a dict with all parameters, use `opts.to_dict()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run CNMF on patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First extract spatial and temporal components on patches and combine them\n",
    "# for this step deconvolution is turned off (p=0)\n",
    "# Then re-run seeded CNMF on accepted patches to refine and perform deconvolution\n",
    "opts.set('temporal', {'p': 0})\n",
    "cnm_list = []\n",
    "\n",
    "t_start = time.time()\n",
    "for ix_plane in range(n_planes):\n",
    "    opts_plane = copy.deepcopy(opts)\n",
    "    if is_dendritic[ix_plane]:\n",
    "        opts_plane.set('init', {'method_init': 'sparse_nmf'})\n",
    "    else:\n",
    "        opts_plane.set('init', {'method_init': 'greedy_roi'})\n",
    "    cnm = cnmf.CNMF(n_processes, params=opts_plane, dview=dview)\n",
    "    cnm.fit(images_list[ix_plane])\n",
    "     \n",
    "    cnm.params.set('temporal', {'p': p})\n",
    "    cnm2 = cnm.refit(images_list[ix_plane], dview=dview)\n",
    "    \n",
    "    cnm_list.append(cnm2)\n",
    "    \n",
    "    clear_output()\n",
    "    \n",
    "    break\n",
    "    \n",
    "t_elapsed = time.time() - t_start\n",
    "print('\\nFinished Source Extract in %1.2f s' % (t_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate components\n",
    "for ix_plane, cnm in enumerate(cnm_list):\n",
    "    cnm.estimates.evaluate_components(images_list[ix_plane], cnm.params, dview=dview)\n",
    "    cnm_list[ix_plane] = cnm\n",
    "    print('\\nPlane %d' % (ix_plane))\n",
    "    print('Found %d good / %d bad components\\n' % (len(cnm.estimates.idx_components), \n",
    "                                                 len(cnm.estimates.idx_components_bad)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot contours of selected and rejected components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plane_ix = 0\n",
    "\n",
    "Cn = cm.local_correlations(images_list[plane_ix].transpose(1,2,0))\n",
    "Cn[np.isnan(Cn)] = 0\n",
    "cnm = cnm_list[plane_ix]\n",
    "cnm.estimates.plot_contours_nb(img=Cn, idx=cnm.estimates.idx_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View traces of accepted and rejected components. Note that if you get data rate error you can start Jupyter notebooks using: 'jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accepted components\n",
    "plane_ix = 0\n",
    "\n",
    "Cn = cm.local_correlations(images_list[plane_ix].transpose(1,2,0))\n",
    "Cn[np.isnan(Cn)] = 0\n",
    "cnm = cnm_list[plane_ix]\n",
    "cnm.estimates.nb_view_components(img=Cn, idx=cnm.estimates.idx_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rejected components\n",
    "plane_ix = 0\n",
    "\n",
    "Cn = cm.local_correlations(images_list[plane_ix].transpose(1,2,0))\n",
    "Cn[np.isnan(Cn)] = 0\n",
    "cnm = cnm_list[plane_ix]\n",
    "if len(cnm.estimates.idx_components_bad) > 0:\n",
    "    cnm.estimates.nb_view_components(img=Cn, idx=cnm.estimates.idx_components_bad)\n",
    "else:\n",
    "    print(\"No components were rejected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plane_ix = 0\n",
    "\n",
    "cnm = cnm_list[plane_ix]\n",
    "# Unravel results\n",
    "A, C, b, f, YrA, S, sn = cnm.estimates.A, cnm.estimates.C, cnm.estimates.b, cnm.estimates.f, \\\n",
    "cnm.estimates.YrA, cnm.estimates.S, cnm.estimates.sn\n",
    "\n",
    "# A   ... n_pixel x n_components sparse matrix (component locations)\n",
    "# C   ... n_component x t np.array (fitted signal)\n",
    "# b   ... ? np.array\n",
    "# f   ... ? np.array (b / f related to global background components)\n",
    "# YrA ... n_component x t np.array (residual)\n",
    "# S   ... deconvolved signal (spike rate(ish))\n",
    "# sn  ... n_pixel np.array (SNR?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot good components on background image and as component map\n",
    "A_dense = A.todense()\n",
    "counter = 1\n",
    "idx_comps = cnm.estimates.idx_components\n",
    "idx_comps_bad = cnm.estimates.idx_components_bad\n",
    "plt.figure(figsize=(20,40));\n",
    "for i_comp in range(len(idx_comps)):\n",
    "    plt.subplot(len(idx_comps),2,counter)\n",
    "    counter += 1\n",
    "    dummy = cm.utils.visualization.plot_contours(A[:,idx_comps[i_comp]], avg_img, cmap='gray', \n",
    "                                                 colors='r', display_numbers=False)\n",
    "    component_img = np.array(np.reshape(A_dense[:,idx_comps[i_comp]], dims, order='F'))\n",
    "    plt.subplot(len(idx_comps),2,counter)\n",
    "    counter += 1\n",
    "    plt.imshow(component_img), plt.title('Component %1.0f' % (i_comp))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before re-classification\n",
    "print('Good components: ')\n",
    "print(idx_comps)\n",
    "print('Bad components: ')\n",
    "print(idx_comps_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_to_exclude = [0,1,2,3,4,5,6,9,10,11,12,14,15,16] # should be index of the good components (i.e. 0,1,2 as shown in plot above)\n",
    "\n",
    "# add to bad components\n",
    "idx_comps_bad = np.sort(np.append(idx_comps_bad, idx_comps[comps_to_exclude]))\n",
    "# remove from good components\n",
    "idx_comps = np.delete(idx_comps, comps_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after re-classification\n",
    "print('Good components: ')\n",
    "print(idx_comps)\n",
    "print('Bad components: ')\n",
    "print(idx_comps_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create component_matrix with good components\n",
    "for i_comp in range(len(idx_comps)):\n",
    "    component_img = np.array(np.reshape(A_dense[:,idx_comps[i_comp]], dims, order='F'))\n",
    "    if i_comp == 0:\n",
    "        component_matrix = component_img\n",
    "    else:\n",
    "        component_matrix = np.dstack((component_matrix, component_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "npz_name = os.path.join(data_folder, '%s_%s_Join_%s_P%d_results_CNMF.npz' % (date_folder, session_folder, group_id, plane_ix))\n",
    "np.savez(npz_name, Cn=Cn, A=A.todense(), C=C, b=b, f=f, YrA=YrA, sn=sn, S=S,\n",
    "         dims=dims, idx_components=idx_comps, idx_components_bad=idx_comps_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract DF/F values and select high-quality components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix_plane, cnm in enumerate(cnm_list):\n",
    "    cnm = cnm_list[plane_ix]\n",
    "    cnm.estimates.detrend_df_f(quantileMin=8, frames_window=250)\n",
    "    cnm.estimates.select_components(use_object=True)\n",
    "    cnm_list[ix_plane] = cnm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accepted components\n",
    "plane_ix = 0\n",
    "\n",
    "Cn = cm.local_correlations(images_list[plane_ix].transpose(1,2,0))\n",
    "Cn[np.isnan(Cn)] = 0\n",
    "cnm = cnm_list[plane_ix]\n",
    "cnm.estimates.nb_view_components(img=Cn, denoised_color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_dff = cnmf_utils.detrend_df_f(A, b, C, f, YrA = YrA, quantileMin=8, frames_window=500)\n",
    "# select good components\n",
    "F_dff = F_dff[idx_comps,:]\n",
    "\n",
    "t = np.arange(0, F_dff.shape[-1]) / frame_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_files = meta['source_file']\n",
    "source_frames = np.array(meta['source_frames'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split up by trials and save as .mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if our numbers match\n",
    "if not (np.sum(source_frames)-len(bad_frames)) == F_dff.shape[-1]:\n",
    "    raise Exception('Sum of source frames minus number of bad frames must be equal to number of timepoints.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caiman",
   "language": "python",
   "name": "caiman"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
