{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CNMF source extraction on movies\n",
    "Step 2 of the Caiman processing pipeline for dendritic two-photon calcium imaging movies. This part uses mmap files as input. These are created during motion correction with the Caiman toolbox (see `01_Preprocess_MC_3D.ipynb`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Setup\n",
    "The first cells import the various Python modules required by the notebook. In particular, a number of modules are imported from the Caiman package. In addition, we also setup the environment so that everything works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic imports\n",
    "# from __future__ import absolute_import, division, print_function\n",
    "# from builtins import *\n",
    "\n",
    "import os, platform, glob, sys, re\n",
    "import fnmatch\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import savemat\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Import Bokeh library\n",
    "from bokeh.plotting import Figure, show\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.models import Range1d, CrosshairTool, HoverTool, Legend\n",
    "from bokeh.io import output_notebook, export_svgs\n",
    "from bokeh.models.sources import ColumnDataSource\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on Linux we have to add the caiman folder to Pythonpath\n",
    "if platform.system() == 'Linux':\n",
    "    sys.path.append(os.path.expanduser('~/caiman'))\n",
    "# environment variables for parallel processing\n",
    "os.environ['MKL_NUM_THREADS']='1'\n",
    "os.environ['OPENBLAS_NUM_THREADS']='1'\n",
    "os.environ['VECLIB_MAXIMUM_THREADS']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CaImAn imports\n",
    "import caiman as cm\n",
    "from caiman.source_extraction.cnmf import cnmf as cnmf\n",
    "from caiman.components_evaluation import estimate_components_quality as estimate_q\n",
    "from caiman.components_evaluation import estimate_components_quality_auto\n",
    "from caiman.utils.visualization import plot_contours, nb_view_patches, nb_plot_contour\n",
    "from caiman.source_extraction.cnmf import utilities as cnmf_utils\n",
    "import caiman_utils as cm_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select files and parameters\n",
    "The following need to be specified:\n",
    "- data_folder ... location of the data (relative to ~/Data)\n",
    "- mc_output ... select if output of rigid ('rig') or piece-wise rigid ('els') motion correction should be used (currently only 'rig' is tested and works)\n",
    "- max_files ... maximum number of files to process, e.g. for testing (if 0, all files will be processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_folder = 'M3_October_2018'\n",
    "date_folder = 'M3_2018-10-02'\n",
    "session_folder = 'S1'\n",
    "group_id = 'G0'\n",
    "\n",
    "mc_output = 'rig'\n",
    "remove_bad_frames = True # remove bad frames specified in Json file\n",
    "\n",
    "# create the complete path to the data folder\n",
    "if platform.system() == 'Linux':\n",
    "    data_folder = '/home/ubuntu/Data'\n",
    "elif platform.system() == 'Darwin':\n",
    "    data_folder = '/Users/Henry/Data/temp/Dendrites_Gwen'\n",
    "data_folder = os.path.join(data_folder, animal_folder, date_folder, session_folder)\n",
    "data_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the mmap file created during motion correction\n",
    "all_files = os.listdir(data_folder)\n",
    "mmap_files = sorted([x for x in all_files if x.startswith('%s_%s' % (date_folder, session_folder)) \n",
    "           and x.endswith('.mmap') and mc_output in x and group_id in x])\n",
    "n_planes = len(mmap_files)\n",
    "\n",
    "print('Found %d mmap files. Check allocation to planes!' % (n_planes))\n",
    "for i_plane in range(n_planes):\n",
    "    print('Plane %d: %s' % (i_plane, mmap_files[i_plane]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmap_files = [os.path.join(data_folder, x) for x in mmap_files]\n",
    "# get metadata\n",
    "for file in os.listdir(data_folder):\n",
    "    if fnmatch.fnmatch(file, '%s_%s_Join_%s_*[!badFrames].json' % (date_folder, session_folder, group_id)):\n",
    "        meta = json.load(open(os.path.join(data_folder,file)))\n",
    "        break\n",
    "trial_index = np.array(meta['trial_index'])\n",
    "frame_rate = meta['frame_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup cluster\n",
    "The default backend mode for parallel processing is through the multiprocessing package. This will allow us to use all the cores in the VM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the cluster (if a cluster already exists terminate it)\n",
    "n_processes = 4 # number of compute processes (None to select automatically)\n",
    "if 'dview' in locals():\n",
    "    dview.terminate()\n",
    "c, dview, n_processes = cm.cluster.setup_cluster(\n",
    "    backend='ipyparallel', n_processes=n_processes, single_thread=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for source extraction\n",
    "Next, we define the important parameters for calcium source extraction. These parameters will have to be iteratively refined for the respective datasets.\n",
    "\n",
    "The parameters are stored in the Python dictionary `params_cnmf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for source extraction and deconvolution\n",
    "decay_time = 0.4            # length of a typical transient in seconds\n",
    "p = 1                       # order of the autoregressive system (normally 1, 2 for fast indicators / imaging)\n",
    "gnb = 2                     # number of global background components\n",
    "merge_thresh = 0.8          # merging threshold, max correlation allowed\n",
    "rf = 10                     # half-size of the patches in pixels. e.g., if rf=25, patches are 50x50 / None: no patches\n",
    "stride_cnmf = 5             # amount of overlap between the patches in pixels\n",
    "K = None                    # number of components per patch (usually None)\n",
    "gSig = [4, 4]               # expected half size of neurons\n",
    "init_method = 'sparse_nmf'  # initialization method (if analyzing dendritic data use 'sparse_nmf')\n",
    "is_dendrites = True         # flag for analyzing dendritic data\n",
    "#alpha_snmf = 10e2           # sparsity penalty for dendritic data analysis through sparse NMF\n",
    "alpha_snmf = 1e-6\n",
    "\n",
    "method_deconvolution='oasis'# deconvolution method (oasis or cvxpy)\n",
    "\n",
    "# parameters for component evaluation\n",
    "min_SNR = 2.5               # signal to noise ratio for accepting a component\n",
    "rval_thr = 0.8              # space correlation threshold for accepting a component\n",
    "use_cnn = False             # whether to use CNN to filter components\n",
    "cnn_thr = 0.8               # threshold for CNN based classifier\n",
    "\n",
    "final_rate = frame_rate             # final frame rate in Hz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run CNMF on joined mmap file\n",
    "According to the Caiman tutorials, CNMF source extraction should be run in several stages. First, we run CNMF on patches of the dataset and then evaluate the quality of the extracted components. Next, CNMF is run again but on the full field-of-view. Finally, the extracted components are again evaluated and classified (good and bad)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load mmap file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBadFramesByTrial(bad_frames, trial_index):\n",
    "\n",
    "    bad_frames_by_trial = dict()\n",
    "    for ix, i_bad in enumerate(bad_frames):\n",
    "        trial_index_bad = trial_index[bad_frames[ix]]\n",
    "        ix_from_trial_start = bad_frames[ix] - np.where(trial_index==trial_index[bad_frames[ix]])[0][0]\n",
    "#         print('Trial / Frame from trial start: %1.0f / %1.0f' % (trial_index_bad, ix_from_trial_start))\n",
    "        if trial_index_bad in bad_frames_by_trial:\n",
    "            bad_frames_by_trial[trial_index_bad] = bad_frames_by_trial[trial_index_bad] + [ix_from_trial_start]\n",
    "        else:\n",
    "            bad_frames_by_trial[trial_index_bad] = [ix_from_trial_start]\n",
    "    \n",
    "    return bad_frames_by_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "Yr, dims, T = cm.load_memmap(fname)\n",
    "d1, d2 = dims\n",
    "\n",
    "# offset if movie is negative\n",
    "if np.min(Yr) < 0:\n",
    "    Yr = Yr - np.min(Yr)\n",
    "    \n",
    "# if file is in F order, convert to C order (required for cnmf)\n",
    "# TODO: check if this can be done more efficienctly by saving directly in C orde\n",
    "if np.isfortran(Yr):\n",
    "    Yr = np.ascontiguousarray(Yr)\n",
    "\n",
    "images = np.reshape(Yr.T, [T] + list(dims), order='F')\n",
    "Y = np.reshape(Yr, dims + (T,), order='F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove bad frames specified in corresponding Json file\n",
    "bad_frames = np.array([])\n",
    "bad_frames_by_trial = dict()\n",
    "if remove_bad_frames:\n",
    "    bad_frames = json.load(open(fname.replace('.mmap','badFrames.json')))\n",
    "    bad_frames = np.array(bad_frames['frames'])\n",
    "    bad_frames_by_trial = getBadFramesByTrial(bad_frames, trial_index)\n",
    "    Yr = np.delete(Yr, bad_frames, axis=1)\n",
    "    trial_index = np.delete(trial_index, bad_frames, axis=0)\n",
    "    T = Yr.shape[1]\n",
    "    images = np.reshape(Yr.T, [T] + list(dims), order='F')\n",
    "    # make sure movie is not negative\n",
    "    add_to_movie = - np.min(images)\n",
    "    fname_new = cm.save_memmap([images], base_name=os.path.join(data_folder, 'removedFrames'), \n",
    "                               add_to_movie=add_to_movie, order='C')\n",
    "    Yr, dims, T = cm.load_memmap(fname_new)\n",
    "    images = np.reshape(Yr.T, [T] + list(dims), order='F')\n",
    "    Y = np.reshape(Yr, dims + (T,), order='F')\n",
    "    print('Deleted %1d frames. Saved to new file %s.' % (len(bad_frames), os.path.basename(fname_new)))\n",
    "    print('Deleted frames:')\n",
    "    print(bad_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  checks on movies (might take time if large!)\n",
    "# if np.min(images) < 0:\n",
    "#     add_to_movie = - np.min(images)\n",
    "#     fname_new = cm.save_memmap([images], base_name=os.path.join(data_folder, 'Yr'), \n",
    "#                                add_to_movie=add_to_movie)\n",
    "#     Yr, dims, T = cm.load_memmap(fname_new)\n",
    "#     images = np.reshape(Yr.T, [T] + list(dims), order='F')\n",
    "#     Y = np.reshape(Yr, dims + (T,), order='F')\n",
    "\n",
    "if np.sum(np.isnan(images)) > 0:\n",
    "    raise Exception('Movie contains nan! You did not remove enough borders')\n",
    "\n",
    "# correlation image\n",
    "Cn = cm.local_correlations(Y)\n",
    "Cn[np.isnan(Cn)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check the image\n",
    "avg_img = np.mean(images,axis=0)\n",
    "plt.figure(figsize=(10,20))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(avg_img, cmap='gray'), plt.title('Frame average');\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(Cn, cmap='jet'), plt.title('Correlation image');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNMF source extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative CNMF first on patches, then full FoV\n",
    "This routine runs an initial CNMF on patches (in parallel), followed by classification into good and bad components. The good components are then re-run through the CNMF algorithm. This procedure follows the suggested workflow in the CaImAn tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cnmf_out, idx_comps, idx_comps_bad = cm_utils.run_cnmf_iterative(images, frame_rate, decay_time, dims, n_processes, K, \n",
    "                                                              gSig, merge_thresh, p, dview, rf, stride_cnmf, \n",
    "                                                              init_method, alpha_snmf, gnb, method_deconvolution, \n",
    "                                                              min_SNR, rval_thr, use_cnn, cnn_thr)\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run single CNMF on full FoV, without patches\n",
    "Alternatively, one can also run a single CNMF on the full FoV. Since there are no patches, the source extraction is not run in parallel. After CNMF, components are evaluated as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnmf_out, idx_comps, idx_comps_bad = cm_utils.run_cnmf_single(images, frame_rate, decay_time, dims, n_processes, K, \n",
    "                                                              gSig, merge_thresh, p, dview, rf, stride_cnmf, \n",
    "                                                              init_method, alpha_snmf, gnb, method_deconvolution, \n",
    "                                                              min_SNR, rval_thr, use_cnn, cnn_thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of detected components')\n",
    "print('Total: %1.0f' % (len(idx_comps)+len(idx_comps_bad)))\n",
    "print('Good: %1.0f' % (len(idx_comps)))\n",
    "print('Bad: %1.0f' % (len(idx_comps_bad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unravel results\n",
    "A, C, b, f, YrA, S, sn = cnmf_out.A, cnmf_out.C, cnmf_out.b, cnmf_out.f, cnmf_out.YrA, cnmf_out.S, cnmf_out.sn\n",
    "# A   ... n_pixel x n_components sparse matrix (component locations)\n",
    "# C   ... n_component x t np.array (fitted signal)\n",
    "# b   ... ? np.array\n",
    "# f   ... ? np.array (b / f related to global background components)\n",
    "# YrA ... n_component x t np.array (residual)\n",
    "# S   ... deconvolved signal (spike rate(ish))\n",
    "# sn  ... n_pixel np.array (SNR?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot good and bad components\n",
    "plt.figure(figsize=(20,30));\n",
    "plt.subplot(121); crd_good = cm.utils.visualization.plot_contours(A[:,idx_comps], Cn, thr=.8, vmax=0.75)\n",
    "plt.title('Contour plots of accepted components')\n",
    "plt.subplot(122); crd_bad = cm.utils.visualization.plot_contours(A[:,idx_comps_bad], Cn, thr=.8, vmax=0.75)\n",
    "plt.title('Contour plots of rejected components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot good components on background image and as component map\n",
    "A_dense = A.todense()\n",
    "counter = 1\n",
    "plt.figure(figsize=(20,40));\n",
    "for i_comp in range(len(idx_comps)):\n",
    "    plt.subplot(len(idx_comps),2,counter)\n",
    "    counter += 1\n",
    "    dummy = cm.utils.visualization.plot_contours(A[:,idx_comps[i_comp]], avg_img, cmap='gray', \n",
    "                                                 colors='r', display_numbers=False)\n",
    "    component_img = np.array(np.reshape(A_dense[:,idx_comps[i_comp]], dims, order='F'))\n",
    "    plt.subplot(len(idx_comps),2,counter)\n",
    "    counter += 1\n",
    "    plt.imshow(component_img), plt.title('Component %1.0f' % (i_comp))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually reclassify components\n",
    "Exclude some 'good' components (select the index, i.e. 0,1,2 as shown in the plot above). These will be added to the list of bad components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before re-classification\n",
    "print('Good components: ')\n",
    "print(idx_comps)\n",
    "print('Bad components: ')\n",
    "print(idx_comps_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_to_exclude = [20,21,22,23,24,25,26,27,28,29,30] # should be index of the good components (i.e. 0,1,2 as shown in plot above)\n",
    "\n",
    "# add to bad components\n",
    "idx_comps_bad = np.sort(np.append(idx_comps_bad, idx_comps[comps_to_exclude]))\n",
    "# remove from good components\n",
    "idx_comps = np.delete(idx_comps, comps_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after re-classification\n",
    "print('Good components: ')\n",
    "print(idx_comps)\n",
    "print('Bad components: ')\n",
    "print(idx_comps_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create component_matrix with good components\n",
    "for i_comp in range(len(idx_comps)):\n",
    "    component_img = np.array(np.reshape(A_dense[:,idx_comps[i_comp]], dims, order='F'))\n",
    "    if i_comp == 0:\n",
    "        component_matrix = component_img\n",
    "    else:\n",
    "        component_matrix = np.dstack((component_matrix, component_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and view results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "npz_name = os.path.join(data_folder, '%s_%s_Join_%s_results_CNMF.npz' % (date_folder, session_folder, group_id))\n",
    "np.savez(npz_name, Cn=Cn, A=A.todense(), C=C, b=b, f=f, YrA=YrA, sn=sn, S=S,\n",
    "         d1=d1, d2=d2, idx_components=idx_comps, idx_components_bad=idx_comps_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interactive viewer for traces of accepted and rejected components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This has to be in a separate cell, otherwise it wont work.\n",
    "from bokeh import resources\n",
    "output_notebook(resources=resources.INLINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accepted components\n",
    "if len(idx_comps) > 0:\n",
    "    nb_view_patches(Yr, A.tocsc()[:, idx_comps], C[idx_comps], \n",
    "                    b, f, dims[0], dims[1], YrA=YrA[idx_comps], image_neurons = Cn,\n",
    "                    denoised_color = 'red');\n",
    "else:\n",
    "    print(\"No accepted components!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate DFF and plot traces\n",
    "The CaImAn function `detrend_df_f` uses a sliding window percentile filter to determine the baseline and compute DFF.\n",
    "Note: for noisy traces and / or high levels of activity, `detrend_df_f` seems to produce sometimes unexpected results (i.e. trace whose shape differs a lot from the extracted component traces). It might be better to use the extracted component traces (see below) for downstream analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_dff = cnmf_utils.detrend_df_f(A, b, C, f, YrA = YrA, quantileMin=8, frames_window=500)\n",
    "# select good components\n",
    "F_dff = F_dff[idx_comps,:]\n",
    "\n",
    "t = np.arange(0, F_dff.shape[-1]) / frame_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract component traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from nb_view_patches\n",
    "YrA_good = YrA[idx_comps] # residual - good components\n",
    "C_good = C[idx_comps] # denoised signal - good components \n",
    "Y_r = C_good + YrA_good # ROI signal - good components\n",
    "S_good = S[idx_comps] # Deconvolved signal - good components\n",
    "x = np.arange(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot DFF and component trace for selected component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_to_plot = 0 # index of component to plot\n",
    "\n",
    "# create the data source for components\n",
    "# Note: division by 100 in nb_view_patches code!\n",
    "source = ColumnDataSource(data=dict(x=x, y=Y_r[comp_to_plot], y2=C_good[comp_to_plot]))\n",
    "\n",
    "p1 = Figure(plot_width=800, plot_height=300, title='Caiman components')\n",
    "# plot ROI signal in blue\n",
    "p1.line('x', 'y', source=source, line_width=1, line_alpha=0.6, color='blue')\n",
    "# plot denoised in red\n",
    "p1.line('x', 'y2', source=source, line_width=1, line_alpha=0.6, color='red')\n",
    "\n",
    "# create the data source for DFF\n",
    "source_dff = ColumnDataSource(data=dict(x=x, y=F_dff[comp_to_plot]))\n",
    "\n",
    "p2 = Figure(plot_width=800, plot_height=300, title='Caiman DFF')\n",
    "# plot ROI signal in blue\n",
    "p2.line('x', 'y', source=source_dff, line_width=1, line_alpha=0.6, color='blue')\n",
    "\n",
    "# make a grid\n",
    "grid = gridplot([[p1], [p2]], sizing_mode='fixed', toolbar_location='left')\n",
    "\n",
    "show(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create stacked plot of all components\n",
    "This plot also shows the trial for each frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_files = meta['source_file']\n",
    "source_frames = np.array(meta['source_frames'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get corresponding trial name for each frame\n",
    "trial_names = [x.replace('_crop.tif','') for x in source_files]\n",
    "trial_names_frames = [trial_names[x] for x in trial_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define some functions for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHover():\n",
    "    \"\"\"Define and return hover tool for a plot\"\"\"\n",
    "    # Define hover tool\n",
    "    hover = HoverTool()\n",
    "    hover.tooltips = [\n",
    "        (\"index\", \"$index\"),\n",
    "        (\"(x,y)\", \"($x, $y)\"),\n",
    "        (\"trial\", \"@trial_idx (@trial_name)\"),\n",
    "    ]\n",
    "    return hover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTimeseries(p, t, y, legend=None, stack=True, xlabel='', ylabel='', output_backend='canvas', \n",
    "                   trial_index=trial_index, trial_names_frames=trial_names_frames):\n",
    "    \"\"\"\n",
    "    Plot a timeseries in Figure p using the Bokeh library\n",
    "    \n",
    "    Input arguments:\n",
    "    p ... Bokeh figure\n",
    "    t ... 1d time axis vector (numpy array)\n",
    "    y ... 2d data numpy array (number of traces x time)\n",
    "    legend ... list of items to be used as figure legend\n",
    "    stack ... whether to stack traces or nor (True / False)\n",
    "    xlabel ... label for x-axis\n",
    "    ylabel ... label for y-axis\n",
    "    output_backend ... 'canvas' or 'svg'\n",
    "    trial_index ... trial index for each frame\n",
    "    trial_names_frames ... trial name for each frame\n",
    "    \"\"\"\n",
    "    \n",
    "    colors_list = ['red', 'green', 'blue', 'yellow', 'cyan', 'orange', 'magenta', 'black', 'gray']\n",
    "    p.add_tools(CrosshairTool(), getHover())\n",
    "    \n",
    "    offset = 0\n",
    "    for i in range(y.shape[0]):\n",
    "        if len(colors_list) < i+1:\n",
    "            colors_list = colors_list + colors_list\n",
    "        \n",
    "        plot_trace = y[i, :]\n",
    "        if stack:\n",
    "            plot_trace = plot_trace - min(plot_trace) + offset\n",
    "            offset = max(plot_trace)\n",
    "        \n",
    "        # create ColumnDataSource\n",
    "        data = {\n",
    "            'x': t, \n",
    "            'y': plot_trace,\n",
    "            'trial_idx': trial_index,\n",
    "            'trial_name': trial_names_frames\n",
    "        }\n",
    "        data_source = ColumnDataSource(data)\n",
    "\n",
    "        # add line\n",
    "        p.line('x', 'y', source=data_source, line_width=2, legend=legend[i], color=colors_list[i])\n",
    "        \n",
    "#     p.legend.location = (0,-30)\n",
    "    p.legend.click_policy=\"hide\"\n",
    "    \n",
    "    # format plot\n",
    "    p.xaxis.axis_label = xlabel\n",
    "    p.yaxis.axis_label = ylabel\n",
    "    \n",
    "    p.x_range = Range1d(np.min(t), np.max(t))\n",
    "    \n",
    "    p.background_fill_color = None\n",
    "    p.border_fill_color = None\n",
    "    \n",
    "    p.output_backend = output_backend\n",
    "\n",
    "    show(p)\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell plots the figure. The function `plotTimeseries` can plot traces on top of each other (`stack=False`) or stacked (`stack=True`). The interactive toolbar on the right of the figures allows panning, zooming, saving etc. One can also hide traces by clicking the corresponding legend item. To save the figure, click the disk icon in the plotting toolbar. With the default `output_backend` ('canvas'), a png file will be saved. To save to svg format, change `output_backend` to 'svg'.\n",
    "\n",
    "Here we can also select to plot only a subset of good components and what type of data to plot (DFF, Y_r or C)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_idx = [0,1,2] # select index of components to plot, e.g. [0,1,2] / use None to plot all components\n",
    "source = 'S_good' # select the data that should be plotted ('F_dff', 'Y_r', 'C_good', 'S_good')\n",
    "\n",
    "if source == 'F_dff':\n",
    "    source_data = F_dff\n",
    "elif source == 'Y_r':\n",
    "    source_data = Y_r\n",
    "elif source == 'C_good':\n",
    "    source_data = C_good\n",
    "elif source == 'S_good':\n",
    "    source_data = S_good\n",
    "else:\n",
    "    raise Exception('Specified source_data is not implemented')\n",
    "\n",
    "if comp_idx is not None:\n",
    "    source_data_plot = source_data[comp_idx,:]\n",
    "    ix = idx_comps[comp_idx]\n",
    "else:\n",
    "    source_data_plot = source_data\n",
    "    ix = idx_comps\n",
    "\n",
    "p = Figure(plot_width=900, plot_height=600, \n",
    "           title=('%s %s CNMF Results' % (date_folder, session_folder)))    \n",
    "legend_text = ['Component %1d' % (x) for x in range(len(ix))]\n",
    "# this is the call to the plotting function (change args. as required)\n",
    "plotTimeseries(p, t, source_data_plot, \n",
    "               legend=legend_text, \n",
    "               stack=True, \n",
    "               xlabel='Time [s]', ylabel=source,\n",
    "               output_backend='canvas',\n",
    "               trial_index=trial_index,\n",
    "               trial_names_frames=trial_names_frames\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split up by trials and save as .mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if our numbers match\n",
    "if not (np.sum(source_frames)-len(bad_frames)) == F_dff.shape[-1]:\n",
    "    raise Exception('Sum of source frames minus number of bad frames must be equal to number of timepoints.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dff = dict()\n",
    "results_Yr = dict()\n",
    "results_C = dict()\n",
    "results_S = dict()\n",
    "removed_frames = dict()\n",
    "for ix, trial_file in enumerate(source_files):\n",
    "    # get indices for current trial's frames\n",
    "    trial_indices = np.where(trial_index==ix)[0]\n",
    "    \n",
    "    if ix in bad_frames_by_trial:\n",
    "        removed_frames_trial = bad_frames_by_trial[ix]\n",
    "    else:\n",
    "        removed_frames_trial = []\n",
    "    \n",
    "    # create a valid Matlab variable / field name\n",
    "    field_name = str('x' + source_files[ix]).replace('.tif','').replace('-','_')\n",
    "    results_dff[field_name] = F_dff[:,trial_indices]\n",
    "    results_Yr[field_name] = Y_r[:,trial_indices]\n",
    "    results_C[field_name] = C_good[:,trial_indices]\n",
    "    results_S[field_name] = S_good[:,trial_indices]\n",
    "    removed_frames[field_name] = removed_frames_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prepare the dictionary for saving as mat file\n",
    "# the field names will be the variable names in Matlab\n",
    "mdict={'trials': [str(x) for x in source_files], \n",
    "       'dff_trial': results_dff,\n",
    "       'Yr_trial': results_Yr,\n",
    "       'C_trial': results_C,\n",
    "       'Deconv_trial': results_S,\n",
    "       'removed_frames': removed_frames,\n",
    "       'mean_image': avg_img,\n",
    "       'spatial_components': component_matrix\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the .mat file\n",
    "matfile_name = os.path.join(data_folder, '%s_%s_Join_%s_results_CNMF.mat' % (date_folder, session_folder, group_id))\n",
    "savemat(os.path.join(data_folder, matfile_name), mdict=mdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caiman",
   "language": "python",
   "name": "caiman"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
