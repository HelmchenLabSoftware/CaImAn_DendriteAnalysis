{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "inputHidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at 'In [29]'.</span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at 'In [29]'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.076324,
     "end_time": "2019-04-05T20:06:29.025008",
     "exception": false,
     "start_time": "2019-04-05T20:06:28.948684",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preprocess and motion correct 3D movies\n",
    "Step 1 of the Caiman processing pipeline for multi-layer two-photon calcium imaging movies. Assume movie format acquired using the Scope setup (i.e. different layers arranged as mosaic on top of each other)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.043024,
     "end_time": "2019-04-05T20:06:29.119221",
     "exception": false,
     "start_time": "2019-04-05T20:06:29.076197",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Imports & Setup\n",
    "The first cells import the various Python modules required by the notebook. In particular, a number of modules are imported from the Caiman package. In addition, we also setup the environment so that everything works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 0.158235,
     "end_time": "2019-04-05T20:06:29.458536",
     "exception": false,
     "start_time": "2019-04-05T20:06:29.300301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generic imports\n",
    "import os, sys, platform, re, math, getpass, shutil, tempfile\n",
    "import json, yaml\n",
    "import time\n",
    "import ipyparallel\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "# import skimage.transform\n",
    "# from tifffile import TiffFile, imread, imsave\n",
    "from tifffile import imsave\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 0.051111,
     "end_time": "2019-04-05T20:06:29.578271",
     "exception": false,
     "start_time": "2019-04-05T20:06:29.527160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# on Linux we have to add the caiman folder to Pythonpath\n",
    "if platform.system() == 'Linux':\n",
    "    sys.path.append(os.path.expanduser('~/caiman'))\n",
    "# environment variables for parallel processing\n",
    "os.environ['MKL_NUM_THREADS']='1'\n",
    "os.environ['OPENBLAS_NUM_THREADS']='1'\n",
    "os.environ['VECLIB_MAXIMUM_THREADS']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 2.122999,
     "end_time": "2019-04-05T20:06:31.741529",
     "exception": false,
     "start_time": "2019-04-05T20:06:29.618530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import CaImAn and custom functions\n",
    "import caiman as cm\n",
    "from caiman.motion_correction import MotionCorrect\n",
    "from caiman.source_extraction.cnmf import params as params\n",
    "import utils, mc_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.040385,
     "end_time": "2019-04-05T20:06:31.824598",
     "exception": false,
     "start_time": "2019-04-05T20:06:31.784213",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Read parameters from config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 0.046832,
     "end_time": "2019-04-05T20:06:31.911348",
     "exception": false,
     "start_time": "2019-04-05T20:06:31.864516",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "config_file = 'config.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 0.047443,
     "end_time": "2019-04-05T20:06:32.000084",
     "exception": false,
     "start_time": "2019-04-05T20:06:31.952641",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "config_file = \"config_M5.2_20181213_S1.yml\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "papermill": {
     "duration": 0.05716,
     "end_time": "2019-04-05T20:06:32.097300",
     "exception": false,
     "start_time": "2019-04-05T20:06:32.040140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'analysis': {'max_group_size': 999, 'max_trials': 999, 'n_planes': 4, 'remove_bad_frames': 1, 'x_crop': 0.5}, 'data': {'animal_folder': 'M5.2', 'area_folder': 'S1', 'connect_storage': 0, 'copy_to_temp': 1, 'data_folder': '/home/luetcke/neurophys-storage/Luetcke/Gwen', 'day_folder': '20181213', 'group_id': 'G0', 'mc_output': 'rig', 'mountpoint': '/home/luetcke/neurophys-storage', 'storage_adress': '//130.60.51.15/Neurophysiology-Storage2', 'storage_user': 'luetcke'}, 'general': {'ncpus': 16}, 'mc': {'border_nan': False, 'max_deviation_rigid': 10, 'niter_rig': 5, 'num_splits_to_process_els': [28, 'None'], 'num_splits_to_process_rig': 'None', 'overlaps': [24, 24], 'pw_rigid': False, 'splits_els': 50, 'splits_rig': 50, 'strides': [24, 24], 'upsample_factor_grid': 4}, 'metrics': {'iters_flow': 3, 'resize_fact_flow': 0.2, 'swap_dim': False, 'winsize': 100}}\n"
     ]
    }
   ],
   "source": [
    "with open(config_file) as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041638,
     "end_time": "2019-04-05T20:06:32.181595",
     "exception": false,
     "start_time": "2019-04-05T20:06:32.139957",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Setup cluster for parallel processing\n",
    "This section starts the IPython cluster (ipcluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "papermill": {
     "duration": 0.047487,
     "end_time": "2019-04-05T20:06:32.268975",
     "exception": false,
     "start_time": "2019-04-05T20:06:32.221488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ncpus = config['general']['ncpus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "papermill": {
     "duration": 10.403256,
     "end_time": "2019-04-05T20:06:42.714264",
     "exception": false,
     "start_time": "2019-04-05T20:06:32.311008",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-05 20:06:36.138 [IPClusterStop] CRITICAL | Could not read pid file, cluster is probably not running.\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$ncpus\"\n",
    "source /opt/Anaconda3-5.1.0-Linux-x86_64/bin/activate caiman || source activate caiman\n",
    "ipcluster stop\n",
    "sleep 5\n",
    "ipcluster start --daemonize -n $1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "papermill": {
     "duration": 12.19351,
     "end_time": "2019-04-05T20:06:54.953043",
     "exception": false,
     "start_time": "2019-04-05T20:06:42.759533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The cluster appears to be setup. Number of parallel processes: 16\n"
     ]
    }
   ],
   "source": [
    "time.sleep(10)\n",
    "# connect client\n",
    "client = ipyparallel.Client()\n",
    "time.sleep(2)\n",
    "while len(client) < ncpus:\n",
    "    sys.stdout.write(\".\")  # Give some visual feedback of things starting\n",
    "    sys.stdout.flush()     # (de-buffered)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# create dview object\n",
    "client.direct_view().execute('__a=1', block=True)\n",
    "dview = client[:]\n",
    "n_processes = len(client)\n",
    "print('\\n\\nThe cluster appears to be setup. Number of parallel processes: %d' % (n_processes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.042551,
     "end_time": "2019-04-05T20:06:55.052752",
     "exception": false,
     "start_time": "2019-04-05T20:06:55.010201",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Map network drive\n",
    "If the data is located on a network drive (i.e. Neurophysiology storage), we first need to connect the drive with the relevant user credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "papermill": {
     "duration": 0.050857,
     "end_time": "2019-04-05T20:06:55.145261",
     "exception": false,
     "start_time": "2019-04-05T20:06:55.094404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "connect_storage = config['data']['connect_storage']\n",
    "if connect_storage:\n",
    "    storage_user = config['data']['storage_user']\n",
    "    storage_adress = config['data']['storage_adress']\n",
    "    mountpoint = config['data']['mountpoint']\n",
    "    storage_pw = getpass.getpass(prompt=\"Enter password for the remote storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "papermill": {
     "duration": 0.049556,
     "end_time": "2019-04-05T20:06:55.239603",
     "exception": false,
     "start_time": "2019-04-05T20:06:55.190047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check if the mountpoint exists, if not create it\n",
    "if connect_storage:\n",
    "    if not os.path.isdir(mountpoint):\n",
    "        os.makedirs(mountpoint)\n",
    "    # list contents of the directory\n",
    "    os.listdir(mountpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "papermill": {
     "duration": 1.325153,
     "end_time": "2019-04-05T20:06:56.605386",
     "exception": false,
     "start_time": "2019-04-05T20:06:55.280233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not mounting storage\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$connect_storage\" \"$storage_user\" \"$storage_pw\" \"$storage_adress\" \"$mountpoint\"\n",
    "if [ \"$1\" = 1 ]; then\n",
    "    sudo mount -t cifs -o username=$2,password=$3,uid=$(id -u),gid=$(id -g) $4 $5\n",
    "else\n",
    "    echo \"Not mounting storage\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "papermill": {
     "duration": 0.054319,
     "end_time": "2019-04-05T20:06:56.727535",
     "exception": false,
     "start_time": "2019-04-05T20:06:56.673216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list contents of the directory\n",
    "if connect_storage:\n",
    "    print('Mountpoint folder content:')\n",
    "    print(os.listdir(mountpoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.040875,
     "end_time": "2019-04-05T20:06:56.811234",
     "exception": false,
     "start_time": "2019-04-05T20:06:56.770359",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Read other parameters from config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "papermill": {
     "duration": 0.05634,
     "end_time": "2019-04-05T20:06:56.912508",
     "exception": false,
     "start_time": "2019-04-05T20:06:56.856168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data parameters\n",
    "data_folder = str(config['data']['data_folder'])\n",
    "animal_folder = str(config['data']['animal_folder'])\n",
    "day_folder = str(config['data']['day_folder'])\n",
    "area_folder = str(config['data']['area_folder'])\n",
    "data_folder = os.path.join(data_folder, animal_folder, day_folder, area_folder)\n",
    "copy_to_temp = bool(config['data']['copy_to_temp'])\n",
    "\n",
    "# analysis parameters\n",
    "max_trials = config['analysis']['max_trials']\n",
    "x_crop = config['analysis']['x_crop']\n",
    "n_planes = config['analysis']['n_planes']\n",
    "max_group_size = config['analysis']['max_group_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041708,
     "end_time": "2019-04-05T20:06:56.998188",
     "exception": false,
     "start_time": "2019-04-05T20:06:56.956480",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Copy data to temporary folder\n",
    "If the data is stored on a remote network location, it is more robust to copy it to a local temporary folder for analysis. Afterwards, the results are copied back to the original location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "papermill": {
     "duration": 0.052274,
     "end_time": "2019-04-05T20:06:57.091708",
     "exception": false,
     "start_time": "2019-04-05T20:06:57.039434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created temporary analysis folder /tmp/tmph1mpi6fm/M5.2/20181213/S1\n"
     ]
    }
   ],
   "source": [
    "if copy_to_temp:\n",
    "    # create a temp directory for analysis\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    # create the data folder structure in the temporary directory\n",
    "    temp_data_folder = os.path.join(temp_dir, animal_folder, day_folder, area_folder)\n",
    "    os.makedirs(temp_data_folder, exist_ok=True)\n",
    "    print('Created temporary analysis folder %s' % (temp_data_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "papermill": {
     "duration": 0.061555,
     "end_time": "2019-04-05T20:06:57.197570",
     "exception": false,
     "start_time": "2019-04-05T20:06:57.136015",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select sessions for processing\n",
    "p = re.compile('\\d\\d-\\d\\d-\\d\\d_Live') # regular expression that should match the folder names (ie. 01-23-45_Live)\n",
    "sessions = [os.path.join(data_folder, x) for (i,x) in enumerate(sorted(os.listdir(data_folder))) if p.match(x) and i <= max_trials]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "papermill": {
     "duration": 95.071873,
     "end_time": "2019-04-05T20:08:32.314263",
     "exception": false,
     "start_time": "2019-04-05T20:06:57.242390",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied data to /tmp/tmph1mpi6fm in 95.02 s (0.75 s per session)\n"
     ]
    }
   ],
   "source": [
    "# copy data to temporary folder\n",
    "if copy_to_temp:\n",
    "    t_start = time.time()\n",
    "    temp_sessions = dview.map_sync(utils.copyDirectory, sessions, [temp_data_folder]*len(sessions))\n",
    "    # print elapsed time\n",
    "    t_elapsed = time.time() - t_start\n",
    "    print('Copied data to %s in %1.2f s (%1.2f s per session)' % (temp_dir, t_elapsed, t_elapsed/len(sessions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "papermill": {
     "duration": 0.051744,
     "end_time": "2019-04-05T20:08:32.417997",
     "exception": false,
     "start_time": "2019-04-05T20:08:32.366253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if copy_to_temp:\n",
    "    sessions = temp_sessions\n",
    "    working_data_folder = temp_data_folder\n",
    "else:\n",
    "    working_data_folder = data_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.042606,
     "end_time": "2019-04-05T20:08:32.504364",
     "exception": false,
     "start_time": "2019-04-05T20:08:32.461758",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Select TIFF files and extract metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "papermill": {
     "duration": 0.059933,
     "end_time": "2019-04-05T20:08:32.609188",
     "exception": false,
     "start_time": "2019-04-05T20:08:32.549255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tiff_files = []\n",
    "xml_files = []\n",
    "for i_session in sessions:\n",
    "    tiff_files.append([os.path.join(i_session, x) for x in os.listdir(i_session) if x.endswith('.tif') and not 'stacked' in x][0])\n",
    "    xml_files.append([os.path.join(i_session, x) for x in os.listdir(i_session) if 'parameters.xml' in x][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "papermill": {
     "duration": 0.800798,
     "end_time": "2019-04-05T20:08:33.473419",
     "exception": false,
     "start_time": "2019-04-05T20:08:32.672621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame rate: 40.8319 Hz (10-09-41_Live)\n",
      "Frame rate: 40.8319 Hz (10-09-55_Live)\n",
      "Frame rate: 40.8319 Hz (10-10-09_Live)\n",
      "Frame rate: 40.8319 Hz (10-10-23_Live)\n",
      "Frame rate: 40.8319 Hz (10-10-37_Live)\n",
      "Frame rate: 40.8319 Hz (10-10-49_Live)\n",
      "Frame rate: 40.8319 Hz (10-11-04_Live)\n",
      "Frame rate: 40.8319 Hz (10-11-18_Live)\n",
      "Frame rate: 40.8319 Hz (10-11-32_Live)\n",
      "Frame rate: 40.8319 Hz (10-11-44_Live)\n",
      "Frame rate: 40.8319 Hz (10-11-56_Live)\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# read frame rate from parameters.xml\n",
    "frame_rates = []\n",
    "for ix, i_session in enumerate(sessions):\n",
    "    tree = ET.parse(xml_files[ix])\n",
    "    root = tree.getroot()\n",
    "    for child in root:\n",
    "        if child.tag == 'area0': # Note: only area 0!\n",
    "            fr = child.find('Framerate_Hz')\n",
    "            frame_rate = float(fr.text)\n",
    "            frame_rates.append(frame_rate)\n",
    "    if ix <= 10:\n",
    "        print('Frame rate: %1.4f Hz (%s)' % (frame_rate, os.path.split(i_session)[1]))\n",
    "if ix > 10:\n",
    "    print('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041842,
     "end_time": "2019-04-05T20:08:33.560914",
     "exception": false,
     "start_time": "2019-04-05T20:08:33.519072",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Convert TIFF files to ImageJ hyperstack files\n",
    "This cell calls the `mosaicToStack` function in `utils` through the ipyparallel `map_sync` method to make use of multiple cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "papermill": {
     "duration": 9.268043,
     "end_time": "2019-04-05T20:08:42.873386",
     "exception": false,
     "start_time": "2019-04-05T20:08:33.605343",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "stacked_files = dview.map_sync(utils.mosaicToStack, tiff_files, [n_planes]*len(tiff_files), [x_crop]*len(tiff_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "papermill": {
     "duration": 0.153433,
     "end_time": "2019-04-05T20:08:43.128688",
     "exception": false,
     "start_time": "2019-04-05T20:08:42.975255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 126 files:\n",
      "/tmp/tmph1mpi6fm/M5.2/20181213/S1/10-09-41_Live/test_A0_Ch0_ 0216_stacked.tif\n",
      "/tmp/tmph1mpi6fm/M5.2/20181213/S1/10-09-55_Live/test_A0_Ch0_ 0217_stacked.tif\n",
      "/tmp/tmph1mpi6fm/M5.2/20181213/S1/10-10-09_Live/test_A0_Ch0_ 0218_stacked.tif\n",
      "/tmp/tmph1mpi6fm/M5.2/20181213/S1/10-10-23_Live/test_A0_Ch0_ 0219_stacked.tif\n",
      "/tmp/tmph1mpi6fm/M5.2/20181213/S1/10-10-37_Live/test_A0_Ch0_ 0220_stacked.tif\n",
      "/tmp/tmph1mpi6fm/M5.2/20181213/S1/10-10-49_Live/test_A0_Ch0_ 0221_stacked.tif\n",
      "/tmp/tmph1mpi6fm/M5.2/20181213/S1/10-11-04_Live/test_A0_Ch0_ 0222_stacked.tif\n",
      "/tmp/tmph1mpi6fm/M5.2/20181213/S1/10-11-18_Live/test_A0_Ch0_ 0223_stacked.tif\n",
      "/tmp/tmph1mpi6fm/M5.2/20181213/S1/10-11-32_Live/test_A0_Ch0_ 0224_stacked.tif\n",
      "/tmp/tmph1mpi6fm/M5.2/20181213/S1/10-11-44_Live/test_A0_Ch0_ 0225_stacked.tif\n",
      "...\n",
      "/tmp/tmph1mpi6fm/M5.2/20181213/S1/10-38-01_Live/test_A0_Ch0_ 0337_stacked.tif\n",
      "/tmp/tmph1mpi6fm/M5.2/20181213/S1/10-38-14_Live/test_A0_Ch0_ 0338_stacked.tif\n",
      "/tmp/tmph1mpi6fm/M5.2/20181213/S1/10-38-28_Live/test_A0_Ch0_ 0339_stacked.tif\n",
      "/tmp/tmph1mpi6fm/M5.2/20181213/S1/10-38-41_Live/test_A0_Ch0_ 0340_stacked.tif\n",
      "/tmp/tmph1mpi6fm/M5.2/20181213/S1/10-38-55_Live/test_A0_Ch0_ 0341_stacked.tif\n"
     ]
    }
   ],
   "source": [
    "print('Processing %1.0f files:' % (len(stacked_files)))\n",
    "print(*stacked_files[:10], sep='\\n')\n",
    "if len(stacked_files) > 10:\n",
    "    print('...')\n",
    "    print(*stacked_files[-5:], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.104786,
     "end_time": "2019-04-05T20:08:43.327846",
     "exception": false,
     "start_time": "2019-04-05T20:08:43.223060",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Join cropped TIF files\n",
    "Next, we create a large joined TIF file from individual cropped files. Further processing will be done on the joined file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "papermill": {
     "duration": 0.789459,
     "end_time": "2019-04-05T20:08:44.253906",
     "exception": false,
     "start_time": "2019-04-05T20:08:43.464447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing files in 1 groups\n",
      "Group 1 (0 - 126): 126 files\n"
     ]
    }
   ],
   "source": [
    "n_groups = math.ceil(len(stacked_files) / float(max_group_size))\n",
    "files_per_group = math.ceil(len(stacked_files) / n_groups)\n",
    "stacked_files_by_group = []\n",
    "print('Processing files in %d groups' % (n_groups))\n",
    "for i_groups in range(int(n_groups)):\n",
    "    start_ix = int(i_groups * files_per_group)\n",
    "    stop_ix = int((i_groups+1) * files_per_group)\n",
    "    stacked_files_by_group.append(stacked_files[start_ix:stop_ix])\n",
    "    \n",
    "    print('Group %d (%d - %d): %d files' % (i_groups+1, start_ix, stop_ix, len(stacked_files[start_ix:stop_ix])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "papermill": {
     "duration": 17.657022,
     "end_time": "2019-04-05T20:09:01.960273",
     "exception": false,
     "start_time": "2019-04-05T20:08:44.303251",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/126 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 2/126 [00:00<00:07, 16.76it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 5/126 [00:00<00:06, 19.17it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 9/126 [00:00<00:05, 21.70it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 13/126 [00:00<00:04, 23.82it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 17/126 [00:00<00:04, 25.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 21/126 [00:00<00:03, 27.48it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|█▉        | 25/126 [00:00<00:03, 28.62it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 28/126 [00:00<00:03, 28.96it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 32/126 [00:01<00:03, 30.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▊       | 36/126 [00:01<00:02, 30.15it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 40/126 [00:01<00:02, 30.77it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▍      | 44/126 [00:01<00:02, 29.90it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 48/126 [00:01<00:02, 30.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|████▏     | 52/126 [00:01<00:02, 26.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████▎     | 55/126 [00:01<00:02, 23.85it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████▌     | 58/126 [00:02<00:02, 22.67it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████▊     | 61/126 [00:02<00:03, 21.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|█████     | 64/126 [00:02<00:02, 21.22it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|█████▎    | 67/126 [00:02<00:02, 20.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████▌    | 70/126 [00:02<00:02, 20.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|█████▊    | 73/126 [00:02<00:02, 20.75it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 76/126 [00:02<00:02, 20.93it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|██████▎   | 79/126 [00:03<00:02, 21.15it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 82/126 [00:03<00:02, 21.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 85/126 [00:03<00:01, 20.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████▉   | 88/126 [00:03<00:01, 20.96it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████▏  | 91/126 [00:03<00:01, 20.88it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▍  | 94/126 [00:03<00:01, 20.66it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████▋  | 97/126 [00:04<00:01, 20.58it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████▉  | 100/126 [00:04<00:01, 20.31it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|████████▏ | 103/126 [00:04<00:01, 18.82it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 105/126 [00:04<00:01, 18.53it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▍ | 107/126 [00:04<00:01, 18.82it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████▋ | 109/126 [00:04<00:00, 17.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 111/126 [00:04<00:00, 17.74it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|████████▉ | 113/126 [00:04<00:00, 17.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|█████████▏| 115/126 [00:05<00:00, 15.99it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|█████████▎| 117/126 [00:05<00:00, 15.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 120/126 [00:05<00:00, 16.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|█████████▋| 122/126 [00:05<00:00, 14.68it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|█████████▊| 124/126 [00:05<00:00, 14.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 126/126 [00:05<00:00, 21.90it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved joined TIF file 20181213_S1_Join_G0_F8974_P0.tif\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved joined TIF file 20181213_S1_Join_G0_F8974_P1.tif\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved joined TIF file 20181213_S1_Join_G0_F8974_P2.tif\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved joined TIF file 20181213_S1_Join_G0_F8974_P3.tif\n",
      "Created JSON metadata file 20181213_S1_Join_G0_F8974.json\n"
     ]
    }
   ],
   "source": [
    "joined_tif_list = []\n",
    "json_fname_list = []\n",
    "total_frames_list = []\n",
    "trial_indices_list = []\n",
    "for i_group, stacked_files_group in enumerate(stacked_files_by_group):\n",
    "    # load movies\n",
    "    movies = cm.load_movie_chain(stacked_files_group, is3D=True, outtype=np.int16)\n",
    "    \n",
    "    total_frames = movies.shape[0]\n",
    "    total_frames_list.append(total_frames)\n",
    "    n_planes = movies.shape[1]\n",
    "    dims = (movies.shape[2], movies.shape[3])\n",
    "    \n",
    "    frames_per_movie = dview.map_sync(utils.getFramesTif, stacked_files_group)\n",
    "    \n",
    "     # trial index for each frame\n",
    "    trial_indices = []\n",
    "    for i_frame, frame_count in enumerate(frames_per_movie):\n",
    "        trial_indices = trial_indices + [i_frame]*frame_count\n",
    "    trial_indices_list.append(trial_indices)\n",
    "    \n",
    "    for i_plane in range(n_planes):\n",
    "        # derive joined file name and save\n",
    "        joined_tif = '%s_%s_Join_G%d_F%d_P%d.tif' % (day_folder, area_folder, i_group, total_frames, i_plane)\n",
    "        imsave(os.path.join(working_data_folder, joined_tif), movies[:, i_plane, :, :])\n",
    "        print('Saved joined TIF file %s' % (joined_tif))\n",
    "    \n",
    "    movies = None # free the memory\n",
    "    \n",
    "    # create a Json file with information about source files\n",
    "    meta = {\"joined_file\": joined_tif.replace('_P%d.tif' % (i_plane), ''), \n",
    "            \"source_frames\": frames_per_movie, \n",
    "            \"source_file\": [x.replace(working_data_folder + os.path.sep,'') for x in stacked_files_group],\n",
    "            \"trial_index\": trial_indices,\n",
    "            \"frame_rate\": frame_rate,\n",
    "            \"z_planes\": n_planes\n",
    "           }\n",
    "    json_fname = joined_tif.replace('_P%d.tif' % (i_plane), '.json')\n",
    "    with open(os.path.join(working_data_folder, json_fname), 'w') as fid:\n",
    "        json.dump(meta, fid)\n",
    "    \n",
    "    # save output file names in list\n",
    "    joined_tif_list.append(joined_tif.replace('_P%d.tif' % (i_plane), ''))\n",
    "    json_fname_list.append(json_fname)\n",
    "    \n",
    "    print('Created JSON metadata file %s' % (json_fname))\n",
    "    \n",
    "# delete stacked TIF files (to save disk space)\n",
    "dummy = dview.map_sync(os.remove, stacked_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.069174,
     "end_time": "2019-04-05T20:09:02.117946",
     "exception": false,
     "start_time": "2019-04-05T20:09:02.048772",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Motion correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.071676,
     "end_time": "2019-04-05T20:09:02.258264",
     "exception": false,
     "start_time": "2019-04-05T20:09:02.186588",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "First, setup the parameters for motion correction. The following parameters influence the **quality** of the motion correction:\n",
    "- niter_rig ... number of iterations for rigid registration (larger = better). Little improvement likely above 5-10.\n",
    "- strides ... intervals at which patches are laid out for motion correction (smaller = better)\n",
    "- overlaps ... overlap between patches\n",
    "\n",
    "Note that smaller values for strides / overlap will improve registration but also lead to NaNs in the output image. In general, there is a trade-off between the quality of registration and the presence / number of NaNs in the output (at least if there is significant motion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "papermill": {
     "duration": 0.080514,
     "end_time": "2019-04-05T20:09:02.404816",
     "exception": false,
     "start_time": "2019-04-05T20:09:02.324302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# some parameters are derived directly from the data\n",
    "max_shifts = (int(np.round(dims[0]/10)), int(np.round(dims[1]/10)))  # maximum allow rigid shift\n",
    "\n",
    "# the rest are read from the config\n",
    "for key in config['mc']:\n",
    "    if isinstance(config['mc'][key], (list,)):\n",
    "        # replace 'None' strings in lists with None\n",
    "        config['mc'][key] = [None if x == 'None' else x for x in config['mc'][key]]\n",
    "    elif isinstance(config['mc'][key], (str,)) and config['mc'][key] == 'None':\n",
    "        config['mc'][key] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "papermill": {
     "duration": 0.205191,
     "end_time": "2019-04-05T20:09:02.678501",
     "exception": false,
     "start_time": "2019-04-05T20:09:02.473310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Changing key niter_rig in group motion from 1 to 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Changing key max_shifts in group motion from (6, 6) to (8, 51)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Changing key splits_rig in group motion from 14 to 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Changing key strides in group motion from (96, 96) to [24, 24]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Changing key overlaps in group motion from (32, 32) to [24, 24]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Changing key splits_els in group motion from 14 to 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Changing key num_splits_to_process_els in group motion from [7, None] to [28, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Changing key max_deviation_rigid in group motion from 3 to 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Changing key border_nan in group motion from copy to False\n"
     ]
    }
   ],
   "source": [
    "# parameters for motion correction\n",
    "opts_dict = {\n",
    "    # number of iterations rigid\n",
    "    'niter_rig': config['mc']['niter_rig'],\n",
    "    # run piecewise-rigid registration?\n",
    "    'pw_rigid': config['mc']['pw_rigid'],\n",
    "    'max_shifts': max_shifts,  # maximum allow rigid shift\n",
    "    # for parallelization split the movies in num_splits chuncks across time\n",
    "    'splits_rig': config['mc']['splits_rig'],\n",
    "    # if none all the splits are processed and the movie is saved\n",
    "    'num_splits_to_process_rig': config['mc']['num_splits_to_process_rig'],\n",
    "    # intervals at which patches are laid out for motion correction\n",
    "    'strides': config['mc']['strides'],\n",
    "    # overlap between patches (size of patch strides+overlaps)\n",
    "    'overlaps': config['mc']['overlaps'],\n",
    "    # for parallelization split the movies in num_splits chuncks across time\n",
    "    'splits_els': config['mc']['splits_els'],\n",
    "    # if none all the splits are processed and the movie is saved\n",
    "    'num_splits_to_process_els': config['mc']['num_splits_to_process_els'],\n",
    "    'upsample_factor_grid': config['mc']['upsample_factor_grid'],  # upsample factor to avoid smearing when merging patches\n",
    "    # maximum deviation allowed for patch with respect to rigid shift\n",
    "    'max_deviation_rigid': config['mc']['max_deviation_rigid'],\n",
    "    # Specifies how to deal with borders. (True, False, 'copy', 'min')\n",
    "    'border_nan': config['mc']['border_nan'],\n",
    "}\n",
    "opts = params.CNMFParams(params_dict=opts_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.069825,
     "end_time": "2019-04-05T20:09:02.819313",
     "exception": false,
     "start_time": "2019-04-05T20:09:02.749488",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There are also some parameters for computing the quality metrics. These probably don't have to be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "papermill": {
     "duration": 0.078045,
     "end_time": "2019-04-05T20:09:02.969119",
     "exception": false,
     "start_time": "2019-04-05T20:09:02.891074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parameters for computing metrics\n",
    "winsize = config['metrics']['winsize']\n",
    "swap_dim = config['metrics']['swap_dim']\n",
    "resize_fact_flow = config['metrics']['resize_fact_flow']    # downsample for computing ROF\n",
    "iters_flow = config['metrics']['iters_flow'] # iterations for calculation of optic flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.071875,
     "end_time": "2019-04-05T20:09:03.112102",
     "exception": false,
     "start_time": "2019-04-05T20:09:03.040227",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next, we define some functions. See the function doc strings for further information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.070621,
     "end_time": "2019-04-05T20:09:03.254899",
     "exception": false,
     "start_time": "2019-04-05T20:09:03.184278",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we are ready to run motion correction for the joined TIF file. If there are a lot of concatenated trials, this might take a while to complete.\n",
    "\n",
    "The following outputs will be saved:\n",
    "- result of rigid motion correction in Python mmap format and as TIF file\n",
    "- result of pw-rigid motion correction in Python mmap format and as TIF file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.069251,
     "end_time": "2019-04-05T20:09:03.393261",
     "exception": false,
     "start_time": "2019-04-05T20:09:03.324010",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Run the motion correction\n",
    "Next, we run the MC. This can be time consuming for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "papermill": {
     "duration": 254.30454,
     "end_time": "2019-04-05T20:13:17.771567",
     "exception": false,
     "start_time": "2019-04-05T20:09:03.467027",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished MC for 20181213_S1_Join_G0_F8974 - Plane 1 of 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished MC for 20181213_S1_Join_G0_F8974 - Plane 2 of 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished MC for 20181213_S1_Join_G0_F8974 - Plane 3 of 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished MC for 20181213_S1_Join_G0_F8974 - Plane 4 of 4\n",
      "\n",
      "Finished MC in 254.22 s (0.01 s per frame)\n"
     ]
    }
   ],
   "source": [
    "t_start = time.time()\n",
    "mc_list = []\n",
    "for ix_file, i_file in enumerate(joined_tif_list):\n",
    "    mc_list.append([])\n",
    "    for i_plane in range(n_planes):\n",
    "        fname = os.path.join(working_data_folder, i_file + '_P%d.tif' % (i_plane))\n",
    "        \n",
    "        if opts.get('motion', 'pw_rigid'):\n",
    "            opts.set('motion', {'pw_rigid': False})\n",
    "            mc = MotionCorrect(fname, dview=dview, **opts.get_group('motion'))\n",
    "            mc.motion_correct(save_movie=True)\n",
    "            fname_rig = mc.fname_tot_rig\n",
    "            opts.set('motion', {'pw_rigid': True})\n",
    "        \n",
    "        mc = MotionCorrect(fname, dview=dview, **opts.get_group('motion'))\n",
    "        mc.motion_correct(save_movie=True)\n",
    "        \n",
    "        if opts.get('motion', 'pw_rigid'):\n",
    "            mc.fname_tot_rig = fname_rig\n",
    "        \n",
    "        # save TIF files\n",
    "        imsave(mc.fname_tot_rig[0].replace('.mmap', '.tif'), cm.load(mc.fname_tot_rig[0]))\n",
    "        if opts.get('motion', 'pw_rigid'):\n",
    "            imsave(mc.fname_tot_els[0].replace('.mmap', '.tif'), cm.load(mc.fname_tot_els[0]))\n",
    "        \n",
    "        mc_list[ix_file].append(mc)\n",
    "        print('Finished MC for %s - Plane %d of %d' % (i_file, i_plane+1, n_planes))\n",
    "\n",
    "# print elapsed time\n",
    "t_elapsed = time.time() - t_start\n",
    "print('\\nFinished MC in %1.2f s (%1.2f s per frame)' % (t_elapsed, t_elapsed/(sum(total_frames_list)*n_planes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.073124,
     "end_time": "2019-04-05T20:13:17.924960",
     "exception": false,
     "start_time": "2019-04-05T20:13:17.851836",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Assess quality of motion correction\n",
    "A number of key metrics can be calculated to assess how much motion correction improved overall motion. \n",
    "1. Correlation\n",
    "Correlations of each frame with the template image (binned median) for original, rigid correction and pw-rigid correction. The mean correlation gives an overall impression of motion. The minimum correlation indicates the parts of the movie that are worst affected by motion. Larger correlations indicate less motion.\n",
    "2. Crispness\n",
    "Crispness provides a measure of the smoothness of the corrected average image. Intuitively, a dataset with nonregistered motion will have a blurred mean image, resulting in a lower value for the total gradient field norm. Thus, larger values indicate a crisper average image and less residual motion. Crispness is calculated from the gradient field of the mean image (`np.gradient`).\n",
    "3. Residual optical flow\n",
    "Optic flow algorithms attempt to match each frame to the template by estimating locally smooth displacement fields. The output is an image where each pixel described the local displacement between template and frame at this point. The smaller the local displacement, the better the registration. Here we compute the matrix norm of the optic flow matrix as summary statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "papermill": {
     "duration": 210.874446,
     "end_time": "2019-04-05T20:16:48.871488",
     "exception": true,
     "start_time": "2019-04-05T20:13:17.997042",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "File size unexpectedly exceeded ZIP64 limit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-9e3149f475e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mmetrics_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_fname_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_group\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_MC_metrics.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     np.savez(os.path.join(working_data_folder, metrics_files[i_group]), metrics=metrics[i_group], crispness=crispness[i_group], norms=norms[i_group], \n\u001b[0;32m---> 42\u001b[0;31m              corr_mean=corr_mean[i_group], corr_min=corr_min[i_group])\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/caiman/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msavez\u001b[0;34m(file, *args, **kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m     \"\"\"\n\u001b[0;32m--> 619\u001b[0;31m     \u001b[0m_savez\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/caiman/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m_savez\u001b[0;34m(file, args, kwds, compress, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m                 format.write_array(fid, val,\n\u001b[1;32m    727\u001b[0m                                    \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m                                    pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    729\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m         \u001b[0;31m# Stage arrays in a temporary file on disk, before writing to zip.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/caiman/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_zip64\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mZIP64_LIMIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m                     raise RuntimeError('File size unexpectedly exceeded ZIP64 '\n\u001b[0m\u001b[1;32m   1044\u001b[0m                                        'limit')\n\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mZIP64_LIMIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: File size unexpectedly exceeded ZIP64 limit"
     ]
    }
   ],
   "source": [
    "# compute quality assessment metrics\n",
    "crispness = []\n",
    "norms = []\n",
    "corr_mean = []\n",
    "corr_min = []\n",
    "metrics = []\n",
    "metrics_files = []\n",
    "for i_group in range(n_groups):\n",
    "    crispness.append([])\n",
    "    norms.append([])\n",
    "    corr_mean.append([])\n",
    "    corr_min.append([])\n",
    "    metrics.append([])\n",
    "    mc_group = mc_list[i_group]\n",
    "    \n",
    "    # remove Pool object (required for parallel processing)\n",
    "    for ix, i_mc in enumerate(mc_group):\n",
    "        i_mc.dview = None\n",
    "        mc_group[ix] = i_mc\n",
    "    \n",
    "    metrics_list = dview.map_sync(mc_utils.computeMetrics, mc_group, [swap_dim]*len(mc_group), [winsize]*len(mc_group), \n",
    "                           [resize_fact_flow]*len(mc_group), [iters_flow]*len(mc_group))\n",
    "    \n",
    "    # collect results\n",
    "    for mtrs in metrics_list:\n",
    "        metrics[i_group].append(mtrs)\n",
    "        if opts.get('motion', 'pw_rigid'):\n",
    "            # correlations, crispness and norms of residual optic flow as indicators of registration quality\n",
    "            crispness[i_group].append(np.array([mtrs['crispness_orig'], mtrs['crispness_rig'], mtrs['crispness_els']]))\n",
    "            norms[i_group].append(np.array([np.mean(mtrs['norms_orig']), np.mean(mtrs['norms_rig']), np.mean(mtrs['norms_els'])]))\n",
    "            corr_mean[i_group].append(np.array([np.mean(mtrs['corr_orig']), np.mean(mtrs['corr_rig']), np.mean(mtrs['corr_els'])]))\n",
    "            corr_min[i_group].append(np.array([np.min(mtrs['corr_orig']), np.min(mtrs['corr_rig']), np.min(mtrs['corr_els'])]))\n",
    "        else:\n",
    "            crispness[i_group].append(np.array([mtrs['crispness_orig'], mtrs['crispness_rig']]))\n",
    "            norms[i_group].append(np.array([np.mean(mtrs['norms_orig']), np.mean(mtrs['norms_rig'])]))\n",
    "            corr_mean[i_group].append(np.array([np.mean(mtrs['corr_orig']), np.mean(mtrs['corr_rig'])]))\n",
    "            corr_min[i_group].append(np.array([np.min(mtrs['corr_orig']), np.min(mtrs['corr_rig'])]))\n",
    "            \n",
    "    # save results\n",
    "    metrics_files.append(json_fname_list[i_group].replace('.json', '_MC_metrics.npz'))\n",
    "    np.savez(os.path.join(working_data_folder, metrics_files[i_group]), metrics=metrics[i_group], crispness=crispness[i_group], norms=norms[i_group], \n",
    "             corr_mean=corr_mean[i_group], corr_min=corr_min[i_group])\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Print different metrics for raw movie and rigid / pw-rigid corrected movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i_group in range(n_groups):\n",
    "    for i_plane in range(n_planes):\n",
    "        print('MC evaluation - Group %d - Plane %d:' % (i_group, i_plane))\n",
    "        mc_utils.printMetrics(corr_mean[i_group][i_plane], corr_min[i_group][i_plane], crispness[i_group][i_plane], norms[i_group][i_plane])\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Create a log for downstream analysis\n",
    "Add important information to the config dictionary and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mc_log = dict()\n",
    "mc_log['config'] = config\n",
    "mc_log['data_folder'] = data_folder\n",
    "mc_log['working_data_folder'] = working_data_folder\n",
    "mc_log['n_groups'] = n_groups\n",
    "mc_log['n_planes'] = n_planes\n",
    "mc_log['tiff_files'] = tiff_files\n",
    "mc_log['joined_tif_list'] = joined_tif_list\n",
    "mc_log['json_fname_list'] = json_fname_list\n",
    "mc_log['metrics_files'] = metrics_files\n",
    "mc_log['stacked_files_by_group'] = stacked_files_by_group\n",
    "mc_log['total_frames_list'] = total_frames_list\n",
    "mc_log['trial_indices_list'] = trial_indices_list\n",
    "\n",
    "# file names of mmap files\n",
    "mmap_files_rig = []\n",
    "mmap_files_els = []\n",
    "for i_group in range(n_groups):\n",
    "    mmap_files_rig.append([])\n",
    "    mmap_files_els.append([])\n",
    "    for i_plane in range(n_planes):\n",
    "        mmap_files_rig[i_group].append(os.path.basename(mc_list[i_group][i_plane].fname_tot_rig[0]))\n",
    "        if config['mc']['pw_rigid']:\n",
    "            mmap_files_els[i_group].append(os.path.basename(mc_list[i_group][i_plane].fname_tot_els[0]))\n",
    "\n",
    "mc_log['mmap_files_rig'] = mmap_files_rig\n",
    "if config['mc']['pw_rigid']:\n",
    "    mc_log['mmap_files_els'] = mmap_files_els\n",
    "            \n",
    "with open(os.path.join(working_data_folder, 'caiman_mc_log.yml'), 'w') as outfile:\n",
    "    yaml.dump(mc_log, outfile, default_flow_style=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Copy back results and delete the temporary folder\n",
    "If copying fails for any reason (e.g. no more disk space), we can still recover the results from the temporary folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if copy_to_temp:\n",
    "    file_types_to_copy = ['.tif', '.npz', '.json', '.mmap', '.yml']\n",
    "    files_to_copy = []\n",
    "    for file_type in file_types_to_copy:\n",
    "        file_list = [x for x in os.listdir(working_data_folder) if not os.path.isdir(x) and x.endswith(file_type)]\n",
    "        files_to_copy = files_to_copy + file_list\n",
    "    out = [shutil.copy2(os.path.join(working_data_folder, x), data_folder) for x in files_to_copy]\n",
    "    \n",
    "    # delete the temp. dir\n",
    "    shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Stop the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "source /opt/Anaconda3-5.1.0-Linux-x86_64/bin/activate caiman || source activate caiman\n",
    "ipcluster stop\n",
    "sleep 1"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "caiman",
   "language": "python",
   "name": "caiman"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "papermill": {
   "duration": 623.380429,
   "end_time": "2019-04-05T20:16:50.476112",
   "environment_variables": {},
   "exception": true,
   "input_path": "01_Preprocess_MC_3D.ipynb",
   "output_path": "01_Preprocess_MC_3D_M5.2_20181213_S1.ibynb",
   "parameters": {
    "config_file": "config_M5.2_20181213_S1.yml"
   },
   "start_time": "2019-04-05T20:06:27.095683",
   "version": "0.19.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}