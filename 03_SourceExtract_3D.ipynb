{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CNMF source extraction on movies\n",
    "Step 2 of the Caiman processing pipeline for dendritic two-photon calcium imaging movies. This part uses mmap files as input. These are created during motion correction with the Caiman toolbox (see `01_Preprocess_MC_3D.ipynb`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Setup\n",
    "The first cells import the various Python modules required by the notebook. In particular, a number of modules are imported from the Caiman package. In addition, we also setup the environment so that everything works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic imports\n",
    "# from __future__ import absolute_import, division, print_function\n",
    "# from builtins import *\n",
    "\n",
    "import os, platform, glob, sys, re, copy, getpass\n",
    "import fnmatch, tempfile, shutil\n",
    "import json, yaml\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import savemat\n",
    "import scipy.spatial.distance as distance\n",
    "from sklearn.decomposition import PCA\n",
    "from tifffile import imsave\n",
    "import subprocess, ipyparallel\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Import Bokeh library\n",
    "import bokeh.plotting as plotting\n",
    "from bokeh.plotting import Figure, show\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.models import Range1d, CrosshairTool, HoverTool, Legend\n",
    "from bokeh.io import output_notebook, export_svgs\n",
    "from bokeh.models.sources import ColumnDataSource\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This has to be in a separate cell, otherwise it wont work.\n",
    "from bokeh import resources\n",
    "output_notebook(resources=resources.INLINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on Linux we have to add the caiman folder to Pythonpath\n",
    "if platform.system() == 'Linux':\n",
    "    sys.path.append(os.path.expanduser('~/caiman'))\n",
    "# environment variables for parallel processing\n",
    "os.environ['MKL_NUM_THREADS']='1'\n",
    "os.environ['OPENBLAS_NUM_THREADS']='1'\n",
    "os.environ['VECLIB_MAXIMUM_THREADS']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CaImAn imports\n",
    "import caiman as cm\n",
    "from caiman.source_extraction.cnmf import cnmf as cnmf\n",
    "from caiman.source_extraction.cnmf import params as params\n",
    "from caiman.components_evaluation import estimate_components_quality as estimate_q\n",
    "from caiman.components_evaluation import estimate_components_quality_auto\n",
    "from caiman.utils.visualization import plot_contours, nb_view_patches, nb_plot_contour\n",
    "from caiman.source_extraction.cnmf import utilities as cnmf_utils\n",
    "import caiman_utils as cm_utils\n",
    "import utils as utils\n",
    "import plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read parameters from config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'config.yml'\n",
    "with open(config_file) as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup cluster for parallel processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section starts the IPython cluster (ipcluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncpus = config['general']['ncpus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$ncpus\"\n",
    "source /opt/Anaconda3-5.1.0-Linux-x86_64/bin/activate caiman || source activate caiman\n",
    "ipcluster stop\n",
    "sleep 5\n",
    "ipcluster start --daemonize -n $1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(10)\n",
    "# connect client\n",
    "client = ipyparallel.Client()\n",
    "time.sleep(2)\n",
    "while len(client) < ncpus:\n",
    "    sys.stdout.write(\".\")  # Give some visual feedback of things starting\n",
    "    sys.stdout.flush()     # (de-buffered)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# create dview object\n",
    "client.direct_view().execute('__a=1', block=True)\n",
    "dview = client[:]\n",
    "n_processes = len(client)\n",
    "print('\\n\\nThe cluster appears to be setup. Number of parallel processes: %d' % (n_processes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map network drive\n",
    "If the data is located on a network drive (i.e. Neurophysiology storage), we first need to connect the drive with the relevant user credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connect_storage = config['data']['connect_storage']\n",
    "if connect_storage:\n",
    "    storage_user = config['data']['storage_user']\n",
    "    storage_adress = config['data']['storage_adress']\n",
    "    mountpoint = config['data']['mountpoint']\n",
    "    storage_pw = getpass.getpass(prompt=\"Enter password for the remote storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the mountpoint exists, if not create it\n",
    "if connect_storage:\n",
    "    if not os.path.isdir(mountpoint):\n",
    "        os.makedirs(mountpoint)\n",
    "    # list contents of the directory\n",
    "    os.listdir(mountpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$connect_storage\" \"$storage_user\" \"$storage_pw\" \"$storage_adress\" \"$mountpoint\"\n",
    "if [ \"$1\" = 1 ]; then\n",
    "    sudo mount -t cifs -o username=$2,password=$3,uid=$(id -u),gid=$(id -g) $4 $5\n",
    "else\n",
    "    echo \"Not mounting storage\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list contents of the directory\n",
    "if connect_storage:\n",
    "    print(os.listdir(mountpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data parameters\n",
    "data_folder = str(config['data']['data_folder'])\n",
    "animal_folder = str(config['data']['animal_folder'])\n",
    "day_folder = str(config['data']['day_folder'])\n",
    "area_folder = str(config['data']['area_folder'])\n",
    "data_folder = os.path.join(data_folder, animal_folder, day_folder, area_folder)\n",
    "copy_to_temp = bool(config['data']['copy_to_temp'])\n",
    "\n",
    "group_id = config['data']['group_id']\n",
    "mc_output = config['data']['mc_output']\n",
    "remove_bad_frames = config['analysis']['remove_bad_frames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metadata\n",
    "for file in os.listdir(data_folder):\n",
    "    if fnmatch.fnmatch(file, '%s_%s_Join_%s_*[!badFrames].json' % (day_folder, area_folder, group_id)):\n",
    "        meta = json.load(open(os.path.join(data_folder,file)))\n",
    "        break\n",
    "trial_index = np.array(meta['trial_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if copy_to_temp:\n",
    "    # create a temp directory for analysis\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    # create the data folder structure in the temporary directory\n",
    "    temp_data_folder = os.path.join(temp_dir, animal_folder, day_folder, area_folder)\n",
    "    os.makedirs(temp_data_folder, exist_ok=True)\n",
    "    print('Created temporary analysis folder %s' % (temp_data_folder))\n",
    "else:\n",
    "    temp_data_folder = data_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select mmap files\n",
    "all_files = os.listdir(data_folder)\n",
    "mmap_files = sorted([x for x in all_files if x.startswith('%s_%s' % (day_folder, area_folder)) \n",
    "           and x.endswith('.mmap') and mc_output in x and group_id in x and not 'remFrames' in x])\n",
    "n_planes = len(mmap_files)\n",
    "\n",
    "print('Found %d mmap files. Check allocation to planes!' % (n_planes))\n",
    "for i_plane in range(n_planes):\n",
    "    print('Plane %d: %s' % (i_plane, mmap_files[i_plane]))\n",
    "mmap_files = [os.path.join(data_folder, x) for x in mmap_files]\n",
    "frame_rate = meta['frame_rate'] / n_planes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy relevant files to temporary analysis folder\n",
    "if copy_to_temp:\n",
    "    t_start = time.time()\n",
    "    \n",
    "    bad_frame_files = [x.replace('.mmap', 'badFrames.json') for x in mmap_files]\n",
    "    files_to_copy = mmap_files + bad_frame_files\n",
    "    out = dview.map_sync(utils.copyFiles, files_to_copy, [temp_data_folder]*len(files_to_copy))\n",
    "    mmap_files_temp = [x.replace(data_folder, temp_data_folder) for x in mmap_files]\n",
    "\n",
    "    t_elapsed = time.time() - t_start\n",
    "    print('Copied %d files to %s in %1.2f s' % (len(files_to_copy), temp_data_folder, t_elapsed))\n",
    "else:\n",
    "    mmap_files_temp = mmap_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and remove bad frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start = time.time()\n",
    "\n",
    "bad_frames = np.array([], dtype='int64')\n",
    "fname_list = []\n",
    "images_list = []\n",
    "\n",
    "# first, create list of bad frame indices (for all planes combined)\n",
    "for fname in mmap_files:\n",
    "    bad_frames = np.concatenate((bad_frames, cm_utils.getBadFrames(fname)))\n",
    "bad_frames = np.unique(bad_frames)\n",
    "\n",
    "# remove the bad frames from all files\n",
    "for fname in mmap_files_temp:\n",
    "    Yr, dims = cm_utils.loadData(fname)\n",
    "    images, Y, fname_rem, bad_frames_by_trial, trial_idx = cm_utils.removeBadFrames(fname, \n",
    "                                                                                      trial_index, \n",
    "                                                                                      Yr, dims, bad_frames, \n",
    "                                                                                      temp_data_folder)\n",
    "    fname_list.append(fname_rem)\n",
    "    images_list.append(images)\n",
    "trial_index = trial_idx\n",
    "\n",
    "t_elapsed = time.time() - t_start\n",
    "print('Loading data / removing frames in %1.2f s' % (t_elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display frame average for each plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,30))\n",
    "for ix_plane in range(n_planes):\n",
    "    avg_img = np.mean(images_list[ix_plane],axis=0)\n",
    "    plt.subplot(n_planes, 1, ix_plane+1)\n",
    "    plt.imshow(avg_img, cmap='gray'), plt.title('Frame average - Plane %d' % (ix_plane), fontsize=32);\n",
    "plt.subplots_adjust(wspace=0, hspace=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export data for manual source extraction\n",
    "The following are exported to the folder where the original data is stored:\n",
    "- 1 TIFF file per plane of motion corrected images with bad frames removed\n",
    "- 1 MAT file per plane that contains:\n",
    "    - motion corrected images with bad frames removed (images)\n",
    "    - trial index for each frame (trial_index)\n",
    "    - list of trial names (trial_names)\n",
    "    - number of frames per trial (trial_frames)\n",
    "    - frame indices of bad frames (bad_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_copy = []\n",
    "\n",
    "bad_frames_by_trial_copy = dict()\n",
    "for key in bad_frames_by_trial.keys():\n",
    "    bad_frames_by_trial_copy['trial_%s' % (key)] = bad_frames_by_trial[key]\n",
    "\n",
    "for ix_plane, images in enumerate(images_list):\n",
    "    # export to TIFF\n",
    "    tiff_name = fname_list[ix_plane].replace('.mmap', '.tif')\n",
    "    files_to_copy.append(tiff_name)\n",
    "    imsave(tiff_name, images)\n",
    "    print('\\nExported TIFF file for plane %d\\n%s' % (ix_plane, tiff_name))\n",
    "    \n",
    "    # export to Matlab\n",
    "    # create dictionary for saving as mat file (field names will be variable names in Matlab)\n",
    "    mdict = {\n",
    "        'images': images,\n",
    "        'trial_index': trial_index,\n",
    "        'trial_names': meta['source_file'],\n",
    "        'trial_frames': meta['source_frames'],\n",
    "        'bad_frames': bad_frames,\n",
    "        'bad_frames_by_trial': bad_frames_by_trial_copy,\n",
    "    }\n",
    "    matfile_name = fname_list[ix_plane].replace('.mmap', '.mat')\n",
    "    files_to_copy.append(matfile_name)\n",
    "    savemat(matfile_name, mdict=mdict, long_field_names=True)\n",
    "    print('\\nExported MAT file for plane %d\\n%s' % (ix_plane, matfile_name))\n",
    "\n",
    "if copy_to_temp:\n",
    "    t_start = time.time()\n",
    "    out = dview.map_sync(utils.copyFiles, files_to_copy, [data_folder]*len(files_to_copy))\n",
    "    print('Copied files to %s in %1.2f s' % (data_folder, time.time()-t_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify if plane contains dendritic signals\n",
    "CaImAn uses different initialization methods depending on whether the signals are dendritic or somatic. Therefore, we need to specify the types of signal expected in each plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_dendritic = [True, True, True, True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for source extraction\n",
    "Next, we define the important parameters for calcium source extraction. These parameters will have to be iteratively refined for the respective datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset dependent parameters\n",
    "decay_time = 0.4                            # length of a typical transient in seconds\n",
    "\n",
    "# parameters for source extraction and deconvolution\n",
    "p = 1                         # order of the autoregressive system\n",
    "gnb = 2                       # number of global background components\n",
    "merge_thresh = 0.8            # merging threshold, max correlation allowed\n",
    "rf = [7,14]                   # half-size of the patches in pixels. e.g., if rf=25, patches are 50x50\n",
    "rf = None\n",
    "stride_cnmf = 3               # amount of overlap between the patches in pixels\n",
    "K = 20                        # max. number of components per patch\n",
    "gSig = [7,35]                 # expected half size of neurons in pixels\n",
    "\n",
    "method_init = 'sparse_nmf'    # initialization method (if analyzing dendritic data use 'sparse_nmf', else 'greedy_roi')\n",
    "#alpha_snmf = 10e2            # sparsity penalty for dendritic data analysis through sparse NMF\n",
    "alpha_snmf = 10\n",
    "normalize_init = True         # default is True\n",
    "sigma_smooth_snmf = (0.5, 0.5, 0.5) # defaults to (0.5, 0.5, 0.5)\n",
    "max_iter_snmf = 500           # defaults to 500\n",
    "\n",
    "ssub = 1                      # spatial subsampling during initialization\n",
    "tsub = 1                      # temporal subsampling during intialization\n",
    "\n",
    "\n",
    "# Parameters for component evaluation\n",
    "quality_params = {\n",
    "    'min_SNR': 3,               # signal to noise ratio for accepting a component\n",
    "    'rval_thr': 0.99,           # space correlation threshold for accepting a component\n",
    "    'use_cnn': False,           # use CNN classifier\n",
    "    'cnn_thr': 0.95,            # threshold for CNN based classifier\n",
    "    'cnn_lowest': 0.1           # neurons with cnn probability lower than this value are rejected\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Parameters object\n",
    "# unspecified parameters get default values\n",
    "opts_dict = {'fnames': fname_list[0],\n",
    "             'fr': frame_rate,\n",
    "            'decay_time': decay_time,\n",
    "            'p': p,\n",
    "            'nb': gnb,\n",
    "            'rf': rf,\n",
    "            'K': K,\n",
    "             'gSig': gSig,\n",
    "            'stride': stride_cnmf,\n",
    "            'method_init': method_init,\n",
    "            'alpha_snmf': alpha_snmf,\n",
    "            'normalize_init': normalize_init,\n",
    "            'sigma_smooth_snmf': sigma_smooth_snmf,\n",
    "            'max_iter_snmf': max_iter_snmf,\n",
    "            'rolling_sum': True,\n",
    "            'only_init': True,\n",
    "            'ssub': ssub,\n",
    "            'tsub': tsub}\n",
    "\n",
    "opts = params.CNMFParams(params_dict=opts_dict)\n",
    "\n",
    "opts.set('quality', quality_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a dict with all parameters, use `opts.to_dict()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run CNMF on patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# First extract spatial and temporal components on patches and combine them\n",
    "# for this step deconvolution is turned off (p=0)\n",
    "# Then re-run seeded CNMF on accepted patches to refine and perform deconvolution\n",
    "opts.set('temporal', {'p': 0})\n",
    "cnm_list = []\n",
    "\n",
    "t_start = time.time()\n",
    "for ix_plane in range(n_planes):\n",
    "    opts_plane = copy.deepcopy(opts)\n",
    "    opts_plane.set('data', {'fnames': [fname_list[ix_plane]]})\n",
    "    if is_dendritic[ix_plane]:\n",
    "        opts_plane.set('init', {'method_init': 'sparse_nmf'})\n",
    "    else:\n",
    "        opts_plane.set('init', {'method_init': 'greedy_roi'})\n",
    "    cnm = cnmf.CNMF(n_processes, params=opts_plane, dview=dview)\n",
    "    cnm.fit(images_list[ix_plane])\n",
    "     \n",
    "    cnm.params.set('temporal', {'p': p})\n",
    "    cnm2 = cnm.refit(images_list[ix_plane], dview=dview)\n",
    "    \n",
    "    cnm_list.append(cnm2)\n",
    "    \n",
    "    clear_output()\n",
    "    \n",
    "t_elapsed = time.time() - t_start\n",
    "print('\\nFinished Source Extract in %1.2f s' % (t_elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix_plane, cnm in enumerate(cnm_list):\n",
    "    opts = copy.deepcopy(cnm.params)\n",
    "    cnm.estimates.evaluate_components(images_list[ix_plane], opts, dview=dview)\n",
    "    cnm_list[ix_plane] = cnm\n",
    "    print('\\nPlane %d' % (ix_plane))\n",
    "    print('Found %d good / %d bad components\\n' % (len(cnm.estimates.idx_components), \n",
    "                                                 len(cnm.estimates.idx_components_bad)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save CNMF results\n",
    "After the time consuming steps of the source extraction are completed, it makes sense to store the results. Variables are stored in the Numpy-specific `.npz` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_basename = '%s_%s_Join_%s_results_CNMF.npz' % (day_folder, area_folder, group_id)\n",
    "npz_name = os.path.join(temp_data_folder, npz_basename)\n",
    "\n",
    "nb_params = {\n",
    "    'data_folder': data_folder,\n",
    "    'day_folder': day_folder,\n",
    "    'area_folder': area_folder,\n",
    "    'group_id': group_id,\n",
    "    'meta': meta,\n",
    "    'trial_index': trial_index,\n",
    "    'bad_frames': bad_frames,\n",
    "    'bad_frames_by_trial': bad_frames_by_trial\n",
    "}\n",
    "\n",
    "for ix_plane, cnm in enumerate(cnm_list):\n",
    "    cnm.dview = None\n",
    "    cnm_list[ix_plane] = cnm\n",
    "np.savez(npz_name, cnm_list=cnm_list, images_list=images_list, nb_params=nb_params)\n",
    "print('Saved CNMF results in %s' % (npz_name))\n",
    "\n",
    "if copy_to_temp:\n",
    "    out = utils.copyFiles(npz_name, data_folder)\n",
    "    print('Copied %s to %s' % (npz_name, data_folder))\n",
    "    print('\\n\\nFor further analysis, load CNMF results from file:\\n', os.path.join(data_folder, npz_basename))\n",
    "else:\n",
    "    print('\\n\\nFor further analysis, load CNMF results from file:\\n', npz_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete temporary folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if copy_to_temp:\n",
    "    # delete the temp. dir\n",
    "    shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source /opt/Anaconda3-5.1.0-Linux-x86_64/bin/activate caiman || source activate caiman\n",
    "ipcluster stop\n",
    "sleep 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CNMF results\n",
    "Load results from a previous CNMF run. If the analysis is continued right away, the load step can be skipped.\n",
    "\n",
    "If you reload the notebook, you have to import the required modules (i.e. run cells 1 - 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data = True\n",
    "\n",
    "if load_data:\n",
    "    npz_name = '/home/luetcke/neurophys-storage/Luetcke/Gwen/M4.3/20181119/S1/20181119_S1_Join_G0_results_CNMF.npz'\n",
    "    npz_content = np.load(npz_name)\n",
    "    nb_params = npz_content['nb_params'][()]\n",
    "    cnm_list = npz_content['cnm_list'][()]\n",
    "    images_list = npz_content['images_list'][()]\n",
    "\n",
    "    print('Loaded file %s (%d planes)' % (os.path.basename(npz_name), len(cnm_list)))\n",
    "else:\n",
    "    nb_params = {\n",
    "        'data_folder': data_folder,\n",
    "        'day_folder': day_folder,\n",
    "        'area_folder': area_folder,\n",
    "        'group_id': group_id,\n",
    "        'meta': meta,\n",
    "        'trial_index': trial_index,\n",
    "        'bad_frames': bad_frames,\n",
    "        'bad_frames_by_trial': bad_frames_by_trial\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-compute local correlations as they will be used a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cn = []\n",
    "for img in images_list:\n",
    "    cc = cm.local_correlations(img.transpose(1,2,0))\n",
    "    cc[np.isnan(cc)] = 0\n",
    "    Cn.append(cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show average images of the different planes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_array = []\n",
    "for ix_plane, image in enumerate(images_list):\n",
    "    grid_array.append([])\n",
    "    image_neurons=np.mean(image,axis=0)\n",
    "    grid_array[ix_plane].append(Figure(plot_width=image_neurons.shape[1]*2, plot_height=image_neurons.shape[0]*2, toolbar_location=\"below\", \n",
    "                                  title=\"Plane %d\" % (ix_plane), x_range = [0, image_neurons.shape[1]], y_range = [0, image_neurons.shape[0]]))\n",
    "    grid_array[ix_plane][0].image(image=[np.flipud(image_neurons)], x=0, y=0, dw=image_neurons.shape[1], dh=image_neurons.shape[0], palette='Greys256')\n",
    "\n",
    "grid = gridplot(grid_array, sizing_mode='fixed', toolbar_location='left')\n",
    "show(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View traces of good or bad components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_or_bad = 'good'\n",
    "plane_ix = 0\n",
    "\n",
    "cnm = cnm_list[plane_ix]\n",
    "images = images_list[plane_ix]\n",
    "if good_or_bad == 'good':\n",
    "    idx = cnm.estimates.idx_components\n",
    "elif good_or_bad == 'bad':\n",
    "    idx = cnm.estimates.idx_components_bad\n",
    "\n",
    "Yr = cnm.estimates.YrA[idx] + cnm.estimates.C[idx]\n",
    "component_images = plotting.nb_view_patches(Yr, idx, cnm.estimates.A, cnm.estimates.C, cnm.estimates.b, cnm.estimates.f, \n",
    "                                             cnm.dims[0], cnm.dims[1], YrA=cnm.estimates.YrA, \n",
    "                                             image_neurons=np.mean(images,axis=0), denoised_color='red', \n",
    "                                             title=\"Plane %d - %s (%d components)\" % (plane_ix, good_or_bad, len(idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing parameters in the CNM object\n",
    "This cell shows how to access relevant parameters stored in the `cnm` object.\n",
    "\n",
    "```python\n",
    "A, C, b, f, YrA, S, sn = cnm.estimates.A, cnm.estimates.C, cnm.estimates.b, cnm.estimates.f, cnm.estimates.YrA, cnm.estimates.S, cnm.estimates.sn\n",
    "```\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Explanation of parameters:**\n",
    "- A   ... n_pixel x n_components sparse matrix (component locations)\n",
    "- C   ... n_component x t np.array (fitted signal)\n",
    "- b   ... ? np.array\n",
    "- f   ... ? np.array (b / f related to global background components)\n",
    "- YrA ... n_component x t np.array (residual)\n",
    "- S   ... deconvolved signal (spike rate(ish))\n",
    "- sn  ... n_pixel np.array (SNR?)\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Convert sparse component matrix to dense matrix:**\n",
    "```python\n",
    "A_dense = A.todense()\n",
    "```\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Indices of good and bad components:**\n",
    "``` python\n",
    "idx_comps = cnm.estimates.idx_components\n",
    "idx_comps_bad = cnm.estimates.idx_components_bad\n",
    "```\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component post-processing\n",
    "Create a plot with good components on background image and as component map. This plot is saved as a PNG file in the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_component_contours(cnm, images, idx_comps, fig_name):\n",
    "    avg_img = np.mean(images,axis=0)\n",
    "    \n",
    "    A = cnm.estimates.A\n",
    "    \n",
    "    try:\n",
    "        A = A.todense()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    counter = 1\n",
    "    plt.figure(figsize=(30,30));\n",
    "    for i_comp in range(len(idx_comps)):\n",
    "        plt.subplot(len(idx_comps),2,counter)\n",
    "        if counter == 1:\n",
    "            plt.title('CNMF Components', fontsize=24);\n",
    "\n",
    "        counter += 1\n",
    "        dummy = cm.utils.visualization.plot_contours(A[:,idx_comps[i_comp]], avg_img, cmap='gray', \n",
    "                                                     colors='r', display_numbers=False)\n",
    "        component_img = np.array(np.reshape(A[:,idx_comps[i_comp]], avg_img.shape, order='F'))\n",
    "        plt.subplot(len(idx_comps),2,counter)\n",
    "        counter += 1\n",
    "        plt.imshow(component_img), plt.title('Component %1.0f' % (i_comp), fontsize=24)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_name)\n",
    "    plt.close()\n",
    "    \n",
    "    print('Saved file %s' % (fig_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix_plane, cnm in enumerate(cnm_list):\n",
    "    idx_comps = cnm.estimates.idx_components\n",
    "    fig_name = os.path.join(nb_params['data_folder'], '%s_%s_Join_%s_P%d_Components_good.png' % \n",
    "                            (nb_params['day_folder'], nb_params['area_folder'], nb_params['group_id'], ix_plane))\n",
    "    plot_component_contours(cnm, images_list[ix_plane], cnm.estimates.idx_components, fig_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove spurious components\n",
    "Try to remove spurious 'good' components consisting of many small spots spread over a large part of the field-of-view. This is done in two ways:\n",
    "1. Calculate component sparsity (i.e. the fraction of pixels with 0)\n",
    "2. The cosine distance between non-zero pixels.\n",
    "\n",
    "For good components, the sparsity should be high (i.e. > 0.99) or the distance between component pixels should be small (i.e. < 0.01)\n",
    "\n",
    "To make the distinction clearer, it helps to threshold the component map before, i.e. at 10% of the max. value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_threshold = 0.1 # threshold at thresh*max before calculating sparsity / cosine distance\n",
    "sparsity_threshold = 0.99 # components with less sparsity will be excluded\n",
    "distance_threshold = 0.01 # components with larger average cosine distance will be excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnm_list_processed = copy.deepcopy(cnm_list)\n",
    "\n",
    "for ix_plane, cnm in enumerate(cnm_list):\n",
    "    A = cnm.estimates.A\n",
    "    try:\n",
    "        A = A.todense()\n",
    "    except:\n",
    "        pass\n",
    "    idx_components = cnm.estimates.idx_components\n",
    "    avg_img = np.mean(images_list[ix_plane],axis=0)\n",
    "    \n",
    "    sparsity = []\n",
    "    dist = []\n",
    "    for ix in idx_components:\n",
    "        component_img = np.array(np.reshape(A[:,ix], avg_img.shape, order='F'))\n",
    "        component_img[component_img < component_threshold*np.max(component_img)] = 0\n",
    "        zeros = np.where(component_img==0)\n",
    "        sparsity.append((zeros[0].shape / np.prod(avg_img.shape))[0])\n",
    "        dist.append(distance.pdist(np.nonzero(component_img), metric='cosine')[0])\n",
    "        \n",
    "    idx_components_proc = [x for (ix,x) in enumerate(idx_components) if sparsity[ix]>sparsity_threshold or dist[ix]<distance_threshold]\n",
    "    \n",
    "    fig_name = os.path.join(nb_params['data_folder'], '%s_%s_Join_%s_P%d_Components_processed.png' % \n",
    "                            (nb_params['day_folder'], nb_params['area_folder'], nb_params['group_id'], ix_plane))\n",
    "    \n",
    "    plot_component_contours(cnm, images_list[ix_plane], idx_components_proc, fig_name)\n",
    "    \n",
    "    cnm_list_processed[ix_plane].estimates.idx_components = idx_components_proc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge correlated components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, get an idea how much different components are correlated. For this, we calculate the cross-correlation between the different spatial components and plot the results for different planes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# component cross-correlation\n",
    "vmin = 0 # min display cutoff\n",
    "vmax = 1 # max display cutoff\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(cnm_list_processed), figsize=(20,20))\n",
    "\n",
    "for ix_plane, cnm in enumerate(cnm_list_processed):\n",
    "    A_dense = cnm.estimates.A.todense()\n",
    "    \n",
    "    idx_components = cnm.estimates.idx_components\n",
    "    cc_mat = np.zeros([len(idx_components), len(idx_components)])\n",
    "    for ix1, c1 in enumerate(idx_components):\n",
    "        for ix2, c2 in enumerate(idx_components):\n",
    "            comp1 = np.array(np.reshape(A_dense[:,c1], [1,np.prod(avg_img.shape)]))\n",
    "            comp2 = np.array(np.reshape(A_dense[:,c2], [1,np.prod(avg_img.shape)]))\n",
    "            cc_mat[ix1,ix2] = np.corrcoef(comp1, comp2)[0][1]\n",
    "\n",
    "    im = axes[ix_plane].imshow(cc_mat, vmin=vmin, vmax=vmax, cmap='jet', aspect='equal')\n",
    "    axes[ix_plane].set_title('Component correlation\\nPlane %d' % (ix_plane), fontsize=16)\n",
    "\n",
    "fig.subplots_adjust(right=0.8)\n",
    "cbar_ax = fig.add_axes([0.85, 0.45, 0.03, 0.1])\n",
    "fig.colorbar(im, cax=cbar_ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run a PCA to determine the additional variance explained by adding components. If some components are highly correlated, then most of the variance should be explained by a subset of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=len(cnm_list_processed), figsize=(20,5))\n",
    "\n",
    "for ix_plane, cnm in enumerate(cnm_list_processed):\n",
    "    A_good = cnm_list_processed[ix_plane].estimates.A.todense()[:,cnm_list_processed[ix_plane].estimates.idx_components]\n",
    "\n",
    "    pca = PCA(n_components=A_good.shape[1])\n",
    "    pca.fit(A_good)\n",
    "\n",
    "    # Plot explained variance\n",
    "    axes[ix_plane].plot(np.arange(1,A_good.shape[1]+1), np.cumsum(pca.explained_variance_ratio_),'o-')\n",
    "    axes[ix_plane].plot([1,A_good.shape[1]+1],[0.9, 0.9],'--')\n",
    "    axes[ix_plane].set_xlabel('number of components')\n",
    "    axes[ix_plane].set_ylabel('cumulative explained variance');\n",
    "    axes[ix_plane].set_title('Explained variance\\nPlane %d' % (ix_plane), fontsize=16)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above plot, we can specify how many components per plane should be kept. 90% explained variance is a suggested cut-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of components to keep per plane\n",
    "components_to_keep = [5, 9, 7, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select spatial components based on PCA components\n",
    "cnm_list_pca = copy.deepcopy(cnm_list_processed)\n",
    "\n",
    "for ix_plane, cnm in enumerate(cnm_list_processed):\n",
    "    \n",
    "    print('\\nPlane: %d' % (ix_plane))\n",
    "    \n",
    "    A = cnm_list_processed[ix_plane].estimates.A\n",
    "    try:\n",
    "        A = A.todense()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    A_good = A[:,cnm_list_processed[ix_plane].estimates.idx_components]\n",
    "    pca = PCA(n_components=components_to_keep[ix_plane])\n",
    "    pca.fit(A_good)\n",
    "    max_idx = np.argmax(pca.components_, axis=1)\n",
    "    \n",
    "    # rearrange components\n",
    "    cnm_list_pca[ix_plane].estimates.idx_components = [cnm_list_processed[ix_plane].estimates.idx_components[x] for x in max_idx]\n",
    "    \n",
    "    fig_name = os.path.join(nb_params['data_folder'], '%s_%s_Join_%s_P%d_Components_pca.png' % \n",
    "                            (nb_params['day_folder'], nb_params['area_folder'], nb_params['group_id'], ix_plane))\n",
    "    \n",
    "    plot_component_contours(cnm_list_pca[ix_plane], images_list[ix_plane], cnm_list_pca[ix_plane].estimates.idx_components, fig_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-inspect traces and components after post-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plane_ix = 0\n",
    "\n",
    "cnm = cnm_list_pca[plane_ix]\n",
    "images = images_list[plane_ix]\n",
    "idx = cnm.estimates.idx_components\n",
    "\n",
    "Yr = cnm.estimates.YrA[idx] + cnm.estimates.C[idx]\n",
    "component_images = plotting.nb_view_patches(Yr, idx, cnm.estimates.A, cnm.estimates.C, cnm.estimates.b, \n",
    "                                            cnm.estimates.f, cnm.dims[0], cnm.dims[1], YrA=cnm.estimates.YrA, \n",
    "                                            image_neurons=np.mean(images,axis=0), denoised_color='red', \n",
    "                                            title=\"Plane %d (%d components)\" % (plane_ix, len(idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create component matrices with good components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_matrix_list = []\n",
    "for ix_plane, cnm in enumerate(cnm_list_pca):\n",
    "    idx_comps = cnm.estimates.idx_components\n",
    "    A = cnm.estimates.A\n",
    "    try:\n",
    "        A = A.todense()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    for i_comp in range(len(idx_comps)):\n",
    "        component_img = np.array(np.reshape(A[:,idx_comps[i_comp]], cnm.dims, order='F'))\n",
    "        if i_comp == 0:\n",
    "            component_matrix = component_img\n",
    "        else:\n",
    "            component_matrix = np.dstack((component_matrix, component_img))\n",
    "            \n",
    "    component_matrix_list.append(component_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract DF/F values\n",
    "The CaImAn function `detrend_df_f` uses a sliding window percentile filter to determine the baseline and compute DFF.\n",
    "Note: for noisy traces and / or high levels of activity, `detrend_df_f` seems to produce sometimes unexpected results (i.e. trace whose shape differs a lot from the extracted component traces). It might be better to use the extracted component traces (see below) for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix_plane, cnm in enumerate(cnm_list_pca):\n",
    "    cnm.estimates.detrend_df_f(quantileMin=8, frames_window=250) # results are in cnm.estimates.F_dff\n",
    "    cnm_list_pca[ix_plane] = cnm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = nb_params['meta']\n",
    "source_files = meta['source_file']\n",
    "source_frames = np.array(meta['source_frames'])\n",
    "trial_index = nb_params['trial_index']\n",
    "frame_rate = meta['frame_rate'] / len(cnm_list_pca)\n",
    "\n",
    "# get corresponding trial name for each frame\n",
    "trial_names = [x.replace('_crop.tif','') for x in source_files]\n",
    "trial_names_frames = [trial_names[x] for x in trial_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create stacked plot of components\n",
    "Plot stacked traces for some or all good components. The source data and plane can be selected. The plot also shows the trial for each frame.\n",
    "Types of source data that can be plotted:\n",
    "- F_dff ... detrended DF/F\n",
    "- YrA ... residual\n",
    "- C ... denoised signal\n",
    "- Y_r ... ROI signal (C + YrA)\n",
    "- S ... Deconvolved signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_idx = None # select index of components to plot, e.g. [0,1,2] / use None to plot all good components\n",
    "source = 'Y_r' # select the data that should be plotted ('F_dff', 'Y_r', 'C', 'S', 'YrA')\n",
    "ix_plane = 0 # select plane that should be plotted\n",
    "\n",
    "cnm = cnm_list_pca[ix_plane]\n",
    "\n",
    "if comp_idx is None:\n",
    "    comp_idx = cnm.estimates.idx_components\n",
    "    print(comp_idx)\n",
    "\n",
    "if source == 'F_dff':\n",
    "    source_data = cnm.estimates.F_dff\n",
    "elif source == 'Y_r':\n",
    "    source_data = cnm.estimates.YrA + cnm.estimates.C\n",
    "elif source == 'YrA':\n",
    "    source_data = cnm.estimates.YrA\n",
    "elif source == 'C':\n",
    "    source_data = cnm.estimates.C\n",
    "elif source == 'S':\n",
    "    source_data = cnm.estimates.S\n",
    "else:\n",
    "    raise Exception('Specified source_data is not implemented')\n",
    "\n",
    "t = np.arange(0, source_data.shape[-1]) / frame_rate\n",
    "    \n",
    "source_data = source_data[comp_idx,:]\n",
    "\n",
    "p = Figure(plot_width=900, plot_height=600, title=('%s %s CNMF Results' % (nb_params['day_folder'], nb_params['area_folder'])))    \n",
    "legend_text = ['Component %d (%d)' % (x, comp_idx[x]) for x in range(source_data.shape[0])]\n",
    "\n",
    "# this is the call to the plotting function (change args. as required)\n",
    "utils.plotTimeseries(p, t, source_data, legend=legend_text, stack=True, xlabel='Time [s]', ylabel=source,\n",
    "                     output_backend='canvas', trial_index=trial_index, trial_names_frames=trial_names_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split up by trials and save as .mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, check if number of frames match\n",
    "if not (np.sum(source_frames)-len(nb_params['bad_frames'])) == cnm_list_pca[0].estimates.F_dff.shape[-1]:\n",
    "    raise Exception('Sum of source frames minus number of bad frames must be equal to number of timepoints.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_frames_by_trial = nb_params['bad_frames_by_trial']\n",
    "\n",
    "for ix_plane, cnm in enumerate(cnm_list_pca):\n",
    "\n",
    "    results_dff = dict()\n",
    "    results_Yr = dict()\n",
    "    results_C = dict()\n",
    "    results_S = dict()\n",
    "    removed_frames = dict()\n",
    "    \n",
    "    comp_idx = cnm.estimates.idx_components\n",
    "    \n",
    "    F_dff = cnm.estimates.F_dff[comp_idx,:]\n",
    "    C = cnm.estimates.C[comp_idx,:]\n",
    "    Y_r = cnm.estimates.YrA[comp_idx,:] + C\n",
    "    S = cnm.estimates.S[comp_idx,:]\n",
    "    \n",
    "    for ix, trial_file in enumerate(source_files):\n",
    "        # get indices for current trial's frames\n",
    "        trial_indices = np.where(trial_index==ix)[0]\n",
    "\n",
    "        if ix in bad_frames_by_trial:\n",
    "            removed_frames_trial = bad_frames_by_trial[ix]\n",
    "        else:\n",
    "            removed_frames_trial = []\n",
    "\n",
    "        # create valid Matlab variables / field names\n",
    "        field_name = str('x' + source_files[ix][:source_files[ix].find('/')]).replace('_Live','').replace('-','_')\n",
    "        results_dff[field_name] = F_dff[:,trial_indices]\n",
    "        results_Yr[field_name] = Y_r[:,trial_indices]\n",
    "        results_C[field_name] = C[:,trial_indices]\n",
    "        results_S[field_name] = S[:,trial_indices]\n",
    "        removed_frames[field_name] = removed_frames_trial\n",
    "        \n",
    "    # dictionary for saving as mat file (field names will be variable names in Matlab)\n",
    "    mdict = {\n",
    "        'trials': [str(x) for x in source_files], \n",
    "        'dff_trial': results_dff,\n",
    "        'Yr_trial': results_Yr,\n",
    "        'C_trial': results_C,\n",
    "        'Deconv_trial': results_S,\n",
    "        'removed_frames': removed_frames,\n",
    "        'mean_image': np.mean(images_list[ix_plane], axis=0),\n",
    "        'spatial_components': component_matrix_list[ix_plane],\n",
    "        'local_correlations': Cn[ix_plane]\n",
    "      }\n",
    "    \n",
    "    # save the .mat file\n",
    "    matfile_name = os.path.join(nb_params['data_folder'], '%s_%s_Join_%s_P%d_results_CNMF.mat' % \n",
    "                                (nb_params['day_folder'], nb_params['area_folder'], nb_params['group_id'], ix_plane))\n",
    "    savemat(os.path.join(nb_params['data_folder'], matfile_name), mdict=mdict, long_field_names=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caiman",
   "language": "python",
   "name": "caiman"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
